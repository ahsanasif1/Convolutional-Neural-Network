{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wjif_5FsdOFz"
   },
   "source": [
    "# SIT744 Assignment 2: Efficient Training of Convolutional Neural Network \n",
    "\n",
    "<div class=\"alert-info\">\n",
    "    <p>Due: <strong>9:00am 18 May 2020</strong>  (Monday)</p>\n",
    "\n",
    "This is an <strong>individual</strong> assignment. It contributes <strong>40%</strong> to your final mark. Read the assignment instruction carefully.\n",
    "\n",
    "<h2> What to submit </h2>\n",
    "\n",
    "<p>\n",
    "This assignment is to be completed individually and submitted to CloudDeakin. <strong>By the due date, you are required to submit the following files to the corresponding Assignment (Dropbox) in CloudDeakin</strong>:\n",
    "\n",
    "<ol>\n",
    "<li>\t<strong>[YourID]_assignment2_solution.ipynp</strong>:  This is your Python notebook solution source file. </li>\n",
    "<li>\t<strong>[YourID]_assingment2_output.html</strong>: This is the output of your Python notebook solution <emph>exported</emph> in HTML format.</li>\n",
    "<li>\tExtra files needed to complete your assignment, if any (e.g., images used in your answers).</li>\n",
    "</ol>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "For example, if your student ID is: 123456, you will then need to submit the following files:\n",
    "<ul>\n",
    "<li> 123456_assignment2_solution.ipynp </li>\n",
    "<li> 123456_assignment2_output.html</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<h2> Warning </h2>\n",
    "\n",
    "Some components of this assignment may involve heavy computation that runs for a long duration. Please start early to avoid missing the assignment due date.\n",
    "\n",
    "<h2> Marking criteria </h2>\n",
    "\n",
    "<p>\n",
    "Your submission will be marked using the following criteria.\n",
    "\n",
    "<ul>\n",
    "<li> Showing good effort through completed tasks.</li>\n",
    "<li> Applying deep learning theory to design suitable deep learning solutions for the tasks.</li>\n",
    "<li> Critically evaluating and reflecting on the pros and cons of various design decisions.</li>\n",
    "<li> Demonstrating creativity and resourcefulness in providing unique individual solutions.</li>\n",
    "<li> Showing attention to details through a good quality assignment report.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Indicative weights of various tasks are provided, but the assignment will be marked by the overall quality per the above criteria.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "twFQbltnm8da"
   },
   "source": [
    "## Assignment objective\n",
    "\n",
    "This assignment is to feedback on your learning in deep learning theory and its application to  data analytics or artificial intelligence problems.  \n",
    "\n",
    "It builds on Assignment 1 but requires a higher level of mastery of deep learning theory and programming/engineering skills. In particular, you will experience training a much deeper network on a large-scale dataset. You will encounter  practical issues that help you consolidate textbook learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "IYJmVdTVDaVx",
    "outputId": "d7fae1d8-c390-468a-94e1-721ecb797ef3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import math\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ITc1hw_o7qV"
   },
   "source": [
    "## Task 1 Solving Fashion-MNIST with Convolutional Neural Networks\n",
    "\n",
    "*(weight ~20%)*\n",
    "\n",
    "In Assignment 1, you tackled the image classification problem in Fashion-MNIST. There, you used a Densely Connected Neural Network. You should now know that is not an optimal model architecture for the problem. In Assignment 2, you will apply the best practices of deep-learning computer vision to improve the image classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdHwmgwOpEfx"
   },
   "source": [
    "### Task 1.1 Revisit Fashion-MNIST classification with DNN\n",
    "\n",
    "*(weight ~3%)*\n",
    "\n",
    "Review your Assignment 1 solution, and reproduce the experiment here. Try to improve the model without changing the model architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfLheAfJjSV0"
   },
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcFF_OrxqmyV"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "#Normalize the data dimensions so that they are of approximately the same scale.\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Further break training data into train / validation sets (# put 5000 into validation set and keep remaining 55,000 for train)\n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000] \n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxrllLPVfzsX"
   },
   "outputs": [],
   "source": [
    "#Making training, testing, and validation pipelines\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(128)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "val_dataset = val_dataset.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_WdjV6aFrISq",
    "outputId": "9c9e3ed4-632e-4817-f628-e0a932a034c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200\n",
      "430/430 - 1s - loss: 0.4144 - accuracy: 0.8594\n",
      "Epoch 9/200\n",
      "430/430 - 1s - loss: 0.4047 - accuracy: 0.8623\n",
      "Epoch 10/200\n",
      "430/430 - 1s - loss: 0.3963 - accuracy: 0.8651\n",
      "Epoch 11/200\n",
      "430/430 - 1s - loss: 0.3888 - accuracy: 0.8675\n",
      "Epoch 12/200\n",
      "430/430 - 1s - loss: 0.3821 - accuracy: 0.8694\n",
      "Epoch 13/200\n",
      "430/430 - 1s - loss: 0.3761 - accuracy: 0.8715\n",
      "Epoch 14/200\n",
      "430/430 - 1s - loss: 0.3705 - accuracy: 0.8732\n",
      "Epoch 15/200\n",
      "430/430 - 1s - loss: 0.3653 - accuracy: 0.8752\n",
      "Epoch 16/200\n",
      "430/430 - 1s - loss: 0.3605 - accuracy: 0.8767\n",
      "Epoch 17/200\n",
      "430/430 - 1s - loss: 0.3560 - accuracy: 0.8782\n",
      "Epoch 18/200\n",
      "430/430 - 1s - loss: 0.3517 - accuracy: 0.8795\n",
      "Epoch 19/200\n",
      "430/430 - 1s - loss: 0.3476 - accuracy: 0.8807\n",
      "Epoch 20/200\n",
      "430/430 - 1s - loss: 0.3436 - accuracy: 0.8817\n",
      "Epoch 21/200\n",
      "430/430 - 1s - loss: 0.3399 - accuracy: 0.8828\n",
      "Epoch 22/200\n",
      "430/430 - 1s - loss: 0.3363 - accuracy: 0.8838\n",
      "Epoch 23/200\n",
      "430/430 - 1s - loss: 0.3328 - accuracy: 0.8845\n",
      "Epoch 24/200\n",
      "430/430 - 1s - loss: 0.3296 - accuracy: 0.8855\n",
      "Epoch 25/200\n",
      "430/430 - 1s - loss: 0.3264 - accuracy: 0.8865\n",
      "Epoch 26/200\n",
      "430/430 - 1s - loss: 0.3234 - accuracy: 0.8876\n",
      "Epoch 27/200\n",
      "430/430 - 1s - loss: 0.3206 - accuracy: 0.8884\n",
      "Epoch 28/200\n",
      "430/430 - 1s - loss: 0.3178 - accuracy: 0.8894\n",
      "Epoch 29/200\n",
      "430/430 - 1s - loss: 0.3151 - accuracy: 0.8900\n",
      "Epoch 30/200\n",
      "430/430 - 1s - loss: 0.3125 - accuracy: 0.8908\n",
      "Epoch 31/200\n",
      "430/430 - 1s - loss: 0.3100 - accuracy: 0.8918\n",
      "Epoch 32/200\n",
      "430/430 - 1s - loss: 0.3075 - accuracy: 0.8926\n",
      "Epoch 33/200\n",
      "430/430 - 1s - loss: 0.3051 - accuracy: 0.8935\n",
      "Epoch 34/200\n",
      "430/430 - 1s - loss: 0.3028 - accuracy: 0.8941\n",
      "Epoch 35/200\n",
      "430/430 - 1s - loss: 0.3005 - accuracy: 0.8949\n",
      "Epoch 36/200\n",
      "430/430 - 1s - loss: 0.2984 - accuracy: 0.8958\n",
      "Epoch 37/200\n",
      "430/430 - 1s - loss: 0.2963 - accuracy: 0.8967\n",
      "Epoch 38/200\n",
      "430/430 - 1s - loss: 0.2942 - accuracy: 0.8971\n",
      "Epoch 39/200\n",
      "430/430 - 1s - loss: 0.2922 - accuracy: 0.8977\n",
      "Epoch 40/200\n",
      "430/430 - 1s - loss: 0.2903 - accuracy: 0.8979\n",
      "Epoch 41/200\n",
      "430/430 - 1s - loss: 0.2883 - accuracy: 0.8984\n",
      "Epoch 42/200\n",
      "430/430 - 1s - loss: 0.2865 - accuracy: 0.8989\n",
      "Epoch 43/200\n",
      "430/430 - 1s - loss: 0.2846 - accuracy: 0.8997\n",
      "Epoch 44/200\n",
      "430/430 - 1s - loss: 0.2829 - accuracy: 0.9004\n",
      "Epoch 45/200\n",
      "430/430 - 1s - loss: 0.2811 - accuracy: 0.9013\n",
      "Epoch 46/200\n",
      "430/430 - 1s - loss: 0.2794 - accuracy: 0.9018\n",
      "Epoch 47/200\n",
      "430/430 - 1s - loss: 0.2777 - accuracy: 0.9025\n",
      "Epoch 48/200\n",
      "430/430 - 1s - loss: 0.2761 - accuracy: 0.9032\n",
      "Epoch 49/200\n",
      "430/430 - 1s - loss: 0.2744 - accuracy: 0.9036\n",
      "Epoch 50/200\n",
      "430/430 - 1s - loss: 0.2728 - accuracy: 0.9043\n",
      "Epoch 51/200\n",
      "430/430 - 1s - loss: 0.2713 - accuracy: 0.9049\n",
      "Epoch 52/200\n",
      "430/430 - 1s - loss: 0.2698 - accuracy: 0.9055\n",
      "Epoch 53/200\n",
      "430/430 - 1s - loss: 0.2683 - accuracy: 0.9059\n",
      "Epoch 54/200\n",
      "430/430 - 1s - loss: 0.2668 - accuracy: 0.9067\n",
      "Epoch 55/200\n",
      "430/430 - 1s - loss: 0.2653 - accuracy: 0.9073\n",
      "Epoch 56/200\n",
      "430/430 - 1s - loss: 0.2639 - accuracy: 0.9079\n",
      "Epoch 57/200\n",
      "430/430 - 1s - loss: 0.2625 - accuracy: 0.9083\n",
      "Epoch 58/200\n",
      "430/430 - 1s - loss: 0.2611 - accuracy: 0.9087\n",
      "Epoch 59/200\n",
      "430/430 - 1s - loss: 0.2598 - accuracy: 0.9091\n",
      "Epoch 60/200\n",
      "430/430 - 1s - loss: 0.2584 - accuracy: 0.9098\n",
      "Epoch 61/200\n",
      "430/430 - 1s - loss: 0.2571 - accuracy: 0.9100\n",
      "Epoch 62/200\n",
      "430/430 - 1s - loss: 0.2557 - accuracy: 0.9105\n",
      "Epoch 63/200\n",
      "430/430 - 1s - loss: 0.2545 - accuracy: 0.9109\n",
      "Epoch 64/200\n",
      "430/430 - 1s - loss: 0.2532 - accuracy: 0.9114\n",
      "Epoch 65/200\n",
      "430/430 - 1s - loss: 0.2519 - accuracy: 0.9117\n",
      "Epoch 66/200\n",
      "430/430 - 1s - loss: 0.2507 - accuracy: 0.9119\n",
      "Epoch 67/200\n",
      "430/430 - 1s - loss: 0.2495 - accuracy: 0.9123\n",
      "Epoch 68/200\n",
      "430/430 - 1s - loss: 0.2483 - accuracy: 0.9128\n",
      "Epoch 69/200\n",
      "430/430 - 1s - loss: 0.2471 - accuracy: 0.9133\n",
      "Epoch 70/200\n",
      "430/430 - 1s - loss: 0.2459 - accuracy: 0.9138\n",
      "Epoch 71/200\n",
      "430/430 - 1s - loss: 0.2447 - accuracy: 0.9141\n",
      "Epoch 72/200\n",
      "430/430 - 1s - loss: 0.2436 - accuracy: 0.9145\n",
      "Epoch 73/200\n",
      "430/430 - 1s - loss: 0.2424 - accuracy: 0.9151\n",
      "Epoch 74/200\n",
      "430/430 - 1s - loss: 0.2413 - accuracy: 0.9154\n",
      "Epoch 75/200\n",
      "430/430 - 1s - loss: 0.2402 - accuracy: 0.9157\n",
      "Epoch 76/200\n",
      "430/430 - 1s - loss: 0.2391 - accuracy: 0.9160\n",
      "Epoch 77/200\n",
      "430/430 - 1s - loss: 0.2380 - accuracy: 0.9163\n",
      "Epoch 78/200\n",
      "430/430 - 1s - loss: 0.2370 - accuracy: 0.9168\n",
      "Epoch 79/200\n",
      "430/430 - 1s - loss: 0.2359 - accuracy: 0.9172\n",
      "Epoch 80/200\n",
      "430/430 - 1s - loss: 0.2348 - accuracy: 0.9175\n",
      "Epoch 81/200\n",
      "430/430 - 1s - loss: 0.2338 - accuracy: 0.9179\n",
      "Epoch 82/200\n",
      "430/430 - 1s - loss: 0.2328 - accuracy: 0.9184\n",
      "Epoch 83/200\n",
      "430/430 - 1s - loss: 0.2317 - accuracy: 0.9188\n",
      "Epoch 84/200\n",
      "430/430 - 1s - loss: 0.2307 - accuracy: 0.9192\n",
      "Epoch 85/200\n",
      "430/430 - 1s - loss: 0.2297 - accuracy: 0.9196\n",
      "Epoch 86/200\n",
      "430/430 - 1s - loss: 0.2287 - accuracy: 0.9199\n",
      "Epoch 87/200\n",
      "430/430 - 1s - loss: 0.2277 - accuracy: 0.9202\n",
      "Epoch 88/200\n",
      "430/430 - 1s - loss: 0.2268 - accuracy: 0.9207\n",
      "Epoch 89/200\n",
      "430/430 - 1s - loss: 0.2258 - accuracy: 0.9211\n",
      "Epoch 90/200\n",
      "430/430 - 1s - loss: 0.2248 - accuracy: 0.9212\n",
      "Epoch 91/200\n",
      "430/430 - 1s - loss: 0.2239 - accuracy: 0.9218\n",
      "Epoch 92/200\n",
      "430/430 - 1s - loss: 0.2230 - accuracy: 0.9220\n",
      "Epoch 93/200\n",
      "430/430 - 1s - loss: 0.2220 - accuracy: 0.9223\n",
      "Epoch 94/200\n",
      "430/430 - 1s - loss: 0.2211 - accuracy: 0.9226\n",
      "Epoch 95/200\n",
      "430/430 - 1s - loss: 0.2202 - accuracy: 0.9230\n",
      "Epoch 96/200\n",
      "430/430 - 1s - loss: 0.2193 - accuracy: 0.9234\n",
      "Epoch 97/200\n",
      "430/430 - 1s - loss: 0.2184 - accuracy: 0.9237\n",
      "Epoch 98/200\n",
      "430/430 - 1s - loss: 0.2174 - accuracy: 0.9241\n",
      "Epoch 99/200\n",
      "430/430 - 1s - loss: 0.2166 - accuracy: 0.9242\n",
      "Epoch 100/200\n",
      "430/430 - 1s - loss: 0.2157 - accuracy: 0.9248\n",
      "Epoch 101/200\n",
      "430/430 - 1s - loss: 0.2148 - accuracy: 0.9252\n",
      "Epoch 102/200\n",
      "430/430 - 1s - loss: 0.2140 - accuracy: 0.9255\n",
      "Epoch 103/200\n",
      "430/430 - 1s - loss: 0.2131 - accuracy: 0.9257\n",
      "Epoch 104/200\n",
      "430/430 - 1s - loss: 0.2123 - accuracy: 0.9259\n",
      "Epoch 105/200\n",
      "430/430 - 1s - loss: 0.2114 - accuracy: 0.9263\n",
      "Epoch 106/200\n",
      "430/430 - 1s - loss: 0.2105 - accuracy: 0.9264\n",
      "Epoch 107/200\n",
      "430/430 - 1s - loss: 0.2097 - accuracy: 0.9267\n",
      "Epoch 108/200\n",
      "430/430 - 1s - loss: 0.2089 - accuracy: 0.9271\n",
      "Epoch 109/200\n",
      "430/430 - 1s - loss: 0.2081 - accuracy: 0.9274\n",
      "Epoch 110/200\n",
      "430/430 - 1s - loss: 0.2073 - accuracy: 0.9278\n",
      "Epoch 111/200\n",
      "430/430 - 1s - loss: 0.2065 - accuracy: 0.9281\n",
      "Epoch 112/200\n",
      "430/430 - 1s - loss: 0.2057 - accuracy: 0.9284\n",
      "Epoch 113/200\n",
      "430/430 - 1s - loss: 0.2049 - accuracy: 0.9286\n",
      "Epoch 114/200\n",
      "430/430 - 1s - loss: 0.2041 - accuracy: 0.9289\n",
      "Epoch 115/200\n",
      "430/430 - 1s - loss: 0.2033 - accuracy: 0.9294\n",
      "Epoch 116/200\n",
      "430/430 - 1s - loss: 0.2025 - accuracy: 0.9296\n",
      "Epoch 117/200\n",
      "430/430 - 1s - loss: 0.2018 - accuracy: 0.9298\n",
      "Epoch 118/200\n",
      "430/430 - 1s - loss: 0.2010 - accuracy: 0.9301\n",
      "Epoch 119/200\n",
      "430/430 - 1s - loss: 0.2002 - accuracy: 0.9303\n",
      "Epoch 120/200\n",
      "430/430 - 1s - loss: 0.1995 - accuracy: 0.9307\n",
      "Epoch 121/200\n",
      "430/430 - 1s - loss: 0.1987 - accuracy: 0.9310\n",
      "Epoch 122/200\n",
      "430/430 - 1s - loss: 0.1980 - accuracy: 0.9314\n",
      "Epoch 123/200\n",
      "430/430 - 1s - loss: 0.1972 - accuracy: 0.9315\n",
      "Epoch 124/200\n",
      "430/430 - 1s - loss: 0.1965 - accuracy: 0.9319\n",
      "Epoch 125/200\n",
      "430/430 - 1s - loss: 0.1958 - accuracy: 0.9325\n",
      "Epoch 126/200\n",
      "430/430 - 1s - loss: 0.1950 - accuracy: 0.9329\n",
      "Epoch 127/200\n",
      "430/430 - 1s - loss: 0.1943 - accuracy: 0.9331\n",
      "Epoch 128/200\n",
      "430/430 - 1s - loss: 0.1936 - accuracy: 0.9334\n",
      "Epoch 129/200\n",
      "430/430 - 1s - loss: 0.1929 - accuracy: 0.9337\n",
      "Epoch 130/200\n",
      "430/430 - 1s - loss: 0.1922 - accuracy: 0.9339\n",
      "Epoch 131/200\n",
      "430/430 - 1s - loss: 0.1915 - accuracy: 0.9342\n",
      "Epoch 132/200\n",
      "430/430 - 1s - loss: 0.1908 - accuracy: 0.9345\n",
      "Epoch 133/200\n",
      "430/430 - 1s - loss: 0.1900 - accuracy: 0.9349\n",
      "Epoch 134/200\n",
      "430/430 - 1s - loss: 0.1894 - accuracy: 0.9350\n",
      "Epoch 135/200\n",
      "430/430 - 1s - loss: 0.1887 - accuracy: 0.9354\n",
      "Epoch 136/200\n",
      "430/430 - 1s - loss: 0.1880 - accuracy: 0.9355\n",
      "Epoch 137/200\n",
      "430/430 - 1s - loss: 0.1873 - accuracy: 0.9359\n",
      "Epoch 138/200\n",
      "430/430 - 1s - loss: 0.1866 - accuracy: 0.9361\n",
      "Epoch 139/200\n",
      "430/430 - 1s - loss: 0.1860 - accuracy: 0.9366\n",
      "Epoch 140/200\n",
      "430/430 - 1s - loss: 0.1853 - accuracy: 0.9366\n",
      "Epoch 141/200\n",
      "430/430 - 1s - loss: 0.1846 - accuracy: 0.9370\n",
      "Epoch 142/200\n",
      "430/430 - 1s - loss: 0.1840 - accuracy: 0.9372\n",
      "Epoch 143/200\n",
      "430/430 - 1s - loss: 0.1833 - accuracy: 0.9373\n",
      "Epoch 144/200\n",
      "430/430 - 1s - loss: 0.1827 - accuracy: 0.9377\n",
      "Epoch 145/200\n",
      "430/430 - 1s - loss: 0.1820 - accuracy: 0.9379\n",
      "Epoch 146/200\n",
      "430/430 - 1s - loss: 0.1814 - accuracy: 0.9382\n",
      "Epoch 147/200\n",
      "430/430 - 1s - loss: 0.1807 - accuracy: 0.9385\n",
      "Epoch 148/200\n",
      "430/430 - 1s - loss: 0.1801 - accuracy: 0.9387\n",
      "Epoch 149/200\n",
      "430/430 - 1s - loss: 0.1795 - accuracy: 0.9389\n",
      "Epoch 150/200\n",
      "430/430 - 1s - loss: 0.1788 - accuracy: 0.9391\n",
      "Epoch 151/200\n",
      "430/430 - 1s - loss: 0.1782 - accuracy: 0.9393\n",
      "Epoch 152/200\n",
      "430/430 - 1s - loss: 0.1776 - accuracy: 0.9396\n",
      "Epoch 153/200\n",
      "430/430 - 1s - loss: 0.1770 - accuracy: 0.9398\n",
      "Epoch 154/200\n",
      "430/430 - 1s - loss: 0.1764 - accuracy: 0.9402\n",
      "Epoch 155/200\n",
      "430/430 - 1s - loss: 0.1758 - accuracy: 0.9403\n",
      "Epoch 156/200\n",
      "430/430 - 1s - loss: 0.1751 - accuracy: 0.9406\n",
      "Epoch 157/200\n",
      "430/430 - 1s - loss: 0.1746 - accuracy: 0.9406\n",
      "Epoch 158/200\n",
      "430/430 - 1s - loss: 0.1739 - accuracy: 0.9410\n",
      "Epoch 159/200\n",
      "430/430 - 1s - loss: 0.1734 - accuracy: 0.9411\n",
      "Epoch 160/200\n",
      "430/430 - 1s - loss: 0.1728 - accuracy: 0.9414\n",
      "Epoch 161/200\n",
      "430/430 - 1s - loss: 0.1722 - accuracy: 0.9416\n",
      "Epoch 162/200\n",
      "430/430 - 1s - loss: 0.1716 - accuracy: 0.9419\n",
      "Epoch 163/200\n",
      "430/430 - 1s - loss: 0.1710 - accuracy: 0.9419\n",
      "Epoch 164/200\n",
      "430/430 - 1s - loss: 0.1704 - accuracy: 0.9422\n",
      "Epoch 165/200\n",
      "430/430 - 1s - loss: 0.1698 - accuracy: 0.9423\n",
      "Epoch 166/200\n",
      "430/430 - 1s - loss: 0.1693 - accuracy: 0.9425\n",
      "Epoch 167/200\n",
      "430/430 - 1s - loss: 0.1687 - accuracy: 0.9427\n",
      "Epoch 168/200\n",
      "430/430 - 1s - loss: 0.1681 - accuracy: 0.9428\n",
      "Epoch 169/200\n",
      "430/430 - 1s - loss: 0.1675 - accuracy: 0.9431\n",
      "Epoch 170/200\n",
      "430/430 - 1s - loss: 0.1669 - accuracy: 0.9436\n",
      "Epoch 171/200\n",
      "430/430 - 1s - loss: 0.1664 - accuracy: 0.9437\n",
      "Epoch 172/200\n",
      "430/430 - 1s - loss: 0.1658 - accuracy: 0.9440\n",
      "Epoch 173/200\n",
      "430/430 - 1s - loss: 0.1652 - accuracy: 0.9442\n",
      "Epoch 174/200\n",
      "430/430 - 1s - loss: 0.1646 - accuracy: 0.9443\n",
      "Epoch 175/200\n",
      "430/430 - 1s - loss: 0.1641 - accuracy: 0.9447\n",
      "Epoch 176/200\n",
      "430/430 - 1s - loss: 0.1635 - accuracy: 0.9447\n",
      "Epoch 177/200\n",
      "430/430 - 1s - loss: 0.1630 - accuracy: 0.9450\n",
      "Epoch 178/200\n",
      "430/430 - 1s - loss: 0.1624 - accuracy: 0.9452\n",
      "Epoch 179/200\n",
      "430/430 - 1s - loss: 0.1618 - accuracy: 0.9454\n",
      "Epoch 180/200\n",
      "430/430 - 1s - loss: 0.1613 - accuracy: 0.9457\n",
      "Epoch 181/200\n",
      "430/430 - 1s - loss: 0.1608 - accuracy: 0.9459\n",
      "Epoch 182/200\n",
      "430/430 - 1s - loss: 0.1602 - accuracy: 0.9461\n",
      "Epoch 183/200\n",
      "430/430 - 1s - loss: 0.1597 - accuracy: 0.9464\n",
      "Epoch 184/200\n",
      "430/430 - 1s - loss: 0.1591 - accuracy: 0.9467\n",
      "Epoch 185/200\n",
      "430/430 - 1s - loss: 0.1586 - accuracy: 0.9468\n",
      "Epoch 186/200\n",
      "430/430 - 1s - loss: 0.1581 - accuracy: 0.9469\n",
      "Epoch 187/200\n",
      "430/430 - 1s - loss: 0.1575 - accuracy: 0.9470\n",
      "Epoch 188/200\n",
      "430/430 - 1s - loss: 0.1570 - accuracy: 0.9472\n",
      "Epoch 189/200\n",
      "430/430 - 1s - loss: 0.1564 - accuracy: 0.9474\n",
      "Epoch 190/200\n",
      "430/430 - 1s - loss: 0.1559 - accuracy: 0.9476\n",
      "Epoch 191/200\n",
      "430/430 - 1s - loss: 0.1554 - accuracy: 0.9480\n",
      "Epoch 192/200\n",
      "430/430 - 1s - loss: 0.1548 - accuracy: 0.9481\n",
      "Epoch 193/200\n",
      "430/430 - 1s - loss: 0.1543 - accuracy: 0.9483\n",
      "Epoch 194/200\n",
      "430/430 - 1s - loss: 0.1538 - accuracy: 0.9485\n",
      "Epoch 195/200\n",
      "430/430 - 1s - loss: 0.1533 - accuracy: 0.9489\n",
      "Epoch 196/200\n",
      "430/430 - 1s - loss: 0.1528 - accuracy: 0.9490\n",
      "Epoch 197/200\n",
      "430/430 - 1s - loss: 0.1522 - accuracy: 0.9492\n",
      "Epoch 198/200\n",
      "430/430 - 1s - loss: 0.1517 - accuracy: 0.9496\n",
      "Epoch 199/200\n",
      "430/430 - 1s - loss: 0.1512 - accuracy: 0.9497\n",
      "Epoch 200/200\n",
      "430/430 - 1s - loss: 0.1507 - accuracy: 0.9499\n"
     ]
    }
   ],
   "source": [
    "#Improved Model by adding the learning rate scheduler and changing the model learning rate to 0.0001 and increased the epochs from 100 to 200\n",
    "\n",
    "model = models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10,activation='softmax')\n",
    "])\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=200, batch_size=128,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "9kou0w7Kr7AI",
    "outputId": "ed907eff-cba8-490d-a23b-131cb0e19ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3416 - accuracy: 0.8856\n",
      "\n",
      "Test accuracy: 88.55999708175659\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test,y_test, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfTL_5kPsl4j"
   },
   "source": [
    "### Task 1.2 Train a ConvNet from scratch\n",
    "\n",
    "*(weight ~5%)*\n",
    "\n",
    "Build a ConvNet to replace the densely connected network in Task 1.1. Report the classification accuracy on the test set. Aim to achieve higher accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "8WFg3Ommcb2p",
    "outputId": "ab0e74ef-c52f-4b83-cd61-7a5f32d5ef2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 32)        8224      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 128)         36992     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               819456    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 867,562\n",
      "Trainable params: 867,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Building a ConvNet from scratch\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Looking at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bFNPa_dKY82b",
    "outputId": "efea740a-93ac-49f7-8057-8be9719a8a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0771 - accuracy: 0.9710 - val_loss: 0.2850 - val_accuracy: 0.9220\n",
      "Epoch 2/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0657 - accuracy: 0.9755 - val_loss: 0.3022 - val_accuracy: 0.9194\n",
      "Epoch 3/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0539 - accuracy: 0.9794 - val_loss: 0.3201 - val_accuracy: 0.9160\n",
      "Epoch 4/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0457 - accuracy: 0.9831 - val_loss: 0.3733 - val_accuracy: 0.9208\n",
      "Epoch 5/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0422 - accuracy: 0.9847 - val_loss: 0.3586 - val_accuracy: 0.9226\n",
      "Epoch 6/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0350 - accuracy: 0.9874 - val_loss: 0.3879 - val_accuracy: 0.9236\n",
      "Epoch 7/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0318 - accuracy: 0.9881 - val_loss: 0.4062 - val_accuracy: 0.9226\n",
      "Epoch 8/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0291 - accuracy: 0.9896 - val_loss: 0.4553 - val_accuracy: 0.9192\n",
      "Epoch 9/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0279 - accuracy: 0.9900 - val_loss: 0.4631 - val_accuracy: 0.9202\n",
      "Epoch 10/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0278 - accuracy: 0.9901 - val_loss: 0.4598 - val_accuracy: 0.9218\n",
      "Epoch 11/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0271 - accuracy: 0.9903 - val_loss: 0.4762 - val_accuracy: 0.9220\n",
      "Epoch 12/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0189 - accuracy: 0.9934 - val_loss: 0.5020 - val_accuracy: 0.9214\n",
      "Epoch 13/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0249 - accuracy: 0.9911 - val_loss: 0.4765 - val_accuracy: 0.9212\n",
      "Epoch 14/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0197 - accuracy: 0.9929 - val_loss: 0.5283 - val_accuracy: 0.9188\n",
      "Epoch 15/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0211 - accuracy: 0.9928 - val_loss: 0.5261 - val_accuracy: 0.9184\n",
      "Epoch 16/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0176 - accuracy: 0.9941 - val_loss: 0.5571 - val_accuracy: 0.9206\n",
      "Epoch 17/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 0.5462 - val_accuracy: 0.9214\n",
      "Epoch 18/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0194 - accuracy: 0.9938 - val_loss: 0.5549 - val_accuracy: 0.9178\n",
      "Epoch 19/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0138 - accuracy: 0.9953 - val_loss: 0.5719 - val_accuracy: 0.9188\n",
      "Epoch 20/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0185 - accuracy: 0.9941 - val_loss: 0.6411 - val_accuracy: 0.9152\n",
      "Epoch 21/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0156 - accuracy: 0.9947 - val_loss: 0.6206 - val_accuracy: 0.9162\n",
      "Epoch 22/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0202 - accuracy: 0.9930 - val_loss: 0.6033 - val_accuracy: 0.9166\n",
      "Epoch 23/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0152 - accuracy: 0.9948 - val_loss: 0.6512 - val_accuracy: 0.9206\n",
      "Epoch 24/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.6328 - val_accuracy: 0.9202\n",
      "Epoch 25/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.6568 - val_accuracy: 0.9140\n",
      "Epoch 26/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0166 - accuracy: 0.9945 - val_loss: 0.6587 - val_accuracy: 0.9194\n",
      "Epoch 27/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.6414 - val_accuracy: 0.9246\n",
      "Epoch 28/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0148 - accuracy: 0.9953 - val_loss: 0.6883 - val_accuracy: 0.9102\n",
      "Epoch 29/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0155 - accuracy: 0.9948 - val_loss: 0.7209 - val_accuracy: 0.9164\n",
      "Epoch 30/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0078 - accuracy: 0.9972 - val_loss: 0.7324 - val_accuracy: 0.9174\n",
      "Epoch 31/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0165 - accuracy: 0.9946 - val_loss: 0.6984 - val_accuracy: 0.9170\n",
      "Epoch 32/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0139 - accuracy: 0.9956 - val_loss: 0.7324 - val_accuracy: 0.9192\n",
      "Epoch 33/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0154 - accuracy: 0.9955 - val_loss: 0.7839 - val_accuracy: 0.9174\n",
      "Epoch 34/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0139 - accuracy: 0.9957 - val_loss: 0.7507 - val_accuracy: 0.9170\n",
      "Epoch 35/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0101 - accuracy: 0.9967 - val_loss: 0.7735 - val_accuracy: 0.9184\n",
      "Epoch 36/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0162 - accuracy: 0.9948 - val_loss: 0.7496 - val_accuracy: 0.9148\n",
      "Epoch 37/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0086 - accuracy: 0.9972 - val_loss: 0.7542 - val_accuracy: 0.9194\n",
      "Epoch 38/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 0.8412 - val_accuracy: 0.9130\n",
      "Epoch 39/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0149 - accuracy: 0.9956 - val_loss: 0.8619 - val_accuracy: 0.9108\n",
      "Epoch 40/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0135 - accuracy: 0.9958 - val_loss: 0.8085 - val_accuracy: 0.9140\n",
      "Epoch 41/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0129 - accuracy: 0.9963 - val_loss: 0.7707 - val_accuracy: 0.9154\n",
      "Epoch 42/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0099 - accuracy: 0.9967 - val_loss: 0.8055 - val_accuracy: 0.9152\n",
      "Epoch 43/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.8820 - val_accuracy: 0.9112\n",
      "Epoch 44/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0133 - accuracy: 0.9962 - val_loss: 0.8292 - val_accuracy: 0.9090\n",
      "Epoch 45/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.8382 - val_accuracy: 0.9120\n",
      "Epoch 46/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.8161 - val_accuracy: 0.9142\n",
      "Epoch 47/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0125 - accuracy: 0.9966 - val_loss: 0.9059 - val_accuracy: 0.9140\n",
      "Epoch 48/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0132 - accuracy: 0.9958 - val_loss: 0.8489 - val_accuracy: 0.9176\n",
      "Epoch 49/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.9319 - val_accuracy: 0.9178\n",
      "Epoch 50/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0140 - accuracy: 0.9959 - val_loss: 0.8625 - val_accuracy: 0.9162\n",
      "Epoch 51/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0103 - accuracy: 0.9971 - val_loss: 0.8628 - val_accuracy: 0.9144\n",
      "Epoch 52/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0095 - accuracy: 0.9974 - val_loss: 0.9609 - val_accuracy: 0.9122\n",
      "Epoch 53/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0150 - accuracy: 0.9960 - val_loss: 0.9266 - val_accuracy: 0.9166\n",
      "Epoch 54/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.9931 - val_accuracy: 0.9138\n",
      "Epoch 55/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.9750 - val_accuracy: 0.9158\n",
      "Epoch 56/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.9230 - val_accuracy: 0.9096\n",
      "Epoch 57/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 1.0336 - val_accuracy: 0.9176\n",
      "Epoch 58/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0109 - accuracy: 0.9971 - val_loss: 0.9769 - val_accuracy: 0.9162\n",
      "Epoch 59/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.9830 - val_accuracy: 0.9124\n",
      "Epoch 60/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.8935 - val_accuracy: 0.9164\n",
      "Epoch 61/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.9790 - val_accuracy: 0.9170\n",
      "Epoch 62/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.9575 - val_accuracy: 0.9160\n",
      "Epoch 63/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0093 - accuracy: 0.9974 - val_loss: 0.9568 - val_accuracy: 0.9146\n",
      "Epoch 64/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.9178 - val_accuracy: 0.9120\n",
      "Epoch 65/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.9946 - val_accuracy: 0.9154\n",
      "Epoch 66/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0155 - accuracy: 0.9960 - val_loss: 0.9362 - val_accuracy: 0.9144\n",
      "Epoch 67/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.9730 - val_accuracy: 0.9134\n",
      "Epoch 68/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.9999 - val_accuracy: 0.9134\n",
      "Epoch 69/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.9479 - val_accuracy: 0.9152\n",
      "Epoch 70/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 1.0268 - val_accuracy: 0.9126\n",
      "Epoch 71/100\n",
      "860/860 [==============================] - 4s 4ms/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 0.9942 - val_accuracy: 0.9154\n",
      "Epoch 72/100\n",
      "860/860 [==============================] - 4s 4ms/step - loss: 0.0096 - accuracy: 0.9978 - val_loss: 0.9184 - val_accuracy: 0.9170\n",
      "Epoch 73/100\n",
      "860/860 [==============================] - 4s 4ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 1.0098 - val_accuracy: 0.9194\n",
      "Epoch 74/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0136 - accuracy: 0.9964 - val_loss: 0.8904 - val_accuracy: 0.9184\n",
      "Epoch 75/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 1.0076 - val_accuracy: 0.9186\n",
      "Epoch 76/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0143 - accuracy: 0.9961 - val_loss: 1.0006 - val_accuracy: 0.9170\n",
      "Epoch 77/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.9813 - val_accuracy: 0.9224\n",
      "Epoch 78/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.9973 - val_accuracy: 0.9126\n",
      "Epoch 79/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 1.0033 - val_accuracy: 0.9214\n",
      "Epoch 80/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0122 - accuracy: 0.9972 - val_loss: 1.0116 - val_accuracy: 0.9188\n",
      "Epoch 81/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 1.0750 - val_accuracy: 0.9146\n",
      "Epoch 82/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0078 - accuracy: 0.9978 - val_loss: 1.0261 - val_accuracy: 0.9194\n",
      "Epoch 83/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0100 - accuracy: 0.9972 - val_loss: 1.1115 - val_accuracy: 0.9170\n",
      "Epoch 84/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0101 - accuracy: 0.9973 - val_loss: 1.1651 - val_accuracy: 0.9132\n",
      "Epoch 85/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 1.0497 - val_accuracy: 0.9140\n",
      "Epoch 86/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 1.1310 - val_accuracy: 0.9152\n",
      "Epoch 87/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 1.1357 - val_accuracy: 0.9170\n",
      "Epoch 88/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 1.2081 - val_accuracy: 0.9138\n",
      "Epoch 89/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 1.1120 - val_accuracy: 0.9180\n",
      "Epoch 90/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 1.0720 - val_accuracy: 0.9154\n",
      "Epoch 91/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0089 - accuracy: 0.9977 - val_loss: 1.1913 - val_accuracy: 0.9192\n",
      "Epoch 92/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 1.1978 - val_accuracy: 0.9160\n",
      "Epoch 93/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 1.2115 - val_accuracy: 0.9162\n",
      "Epoch 94/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0094 - accuracy: 0.9977 - val_loss: 1.1913 - val_accuracy: 0.9208\n",
      "Epoch 95/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 1.1963 - val_accuracy: 0.9170\n",
      "Epoch 96/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0081 - accuracy: 0.9979 - val_loss: 1.2957 - val_accuracy: 0.9148\n",
      "Epoch 97/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0087 - accuracy: 0.9977 - val_loss: 1.2391 - val_accuracy: 0.9190\n",
      "Epoch 98/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 1.2306 - val_accuracy: 0.9144\n",
      "Epoch 99/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 1.2724 - val_accuracy: 0.9124\n",
      "Epoch 100/100\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.0117 - accuracy: 0.9972 - val_loss: 1.2444 - val_accuracy: 0.9204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f98600814a8>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=64,\n",
    "         epochs=100,\n",
    "         validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "SbTvRHbEY86v",
    "outputId": "06d8ae28-3b46-4649-ca28-8b340ef75e6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 91.430002450943\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "colab_type": "code",
    "id": "kib5D8WpY89r",
    "outputId": "20d85491-df4f-4fe7-c7ef-ebf2d5a62752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 5, 5, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               819456    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 914,698\n",
      "Trainable params: 914,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Attempt to make the current cnn achieve better accuracy \n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IScwyu8j5MyR",
    "outputId": "6e2c47bc-3987-45c0-da35-9df1890b7c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/430 [==============================] - 2s 5ms/step - loss: 0.2588 - accuracy: 0.9039 - val_loss: 0.2203 - val_accuracy: 0.9188\n",
      "Epoch 7/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.2506 - accuracy: 0.9081 - val_loss: 0.2164 - val_accuracy: 0.9184\n",
      "Epoch 8/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.2375 - accuracy: 0.9118 - val_loss: 0.2035 - val_accuracy: 0.9242\n",
      "Epoch 9/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.2287 - accuracy: 0.9146 - val_loss: 0.2197 - val_accuracy: 0.9182\n",
      "Epoch 10/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.2195 - accuracy: 0.9171 - val_loss: 0.2009 - val_accuracy: 0.9268\n",
      "Epoch 11/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.2101 - accuracy: 0.9219 - val_loss: 0.2000 - val_accuracy: 0.9268\n",
      "Epoch 12/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.2092 - accuracy: 0.9219 - val_loss: 0.2002 - val_accuracy: 0.9284\n",
      "Epoch 13/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.2028 - accuracy: 0.9243 - val_loss: 0.1948 - val_accuracy: 0.9262\n",
      "Epoch 14/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1953 - accuracy: 0.9274 - val_loss: 0.1827 - val_accuracy: 0.9328\n",
      "Epoch 15/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1899 - accuracy: 0.9285 - val_loss: 0.1887 - val_accuracy: 0.9292\n",
      "Epoch 16/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1854 - accuracy: 0.9301 - val_loss: 0.1865 - val_accuracy: 0.9352\n",
      "Epoch 17/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1813 - accuracy: 0.9319 - val_loss: 0.1818 - val_accuracy: 0.9332\n",
      "Epoch 18/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1747 - accuracy: 0.9335 - val_loss: 0.1848 - val_accuracy: 0.9330\n",
      "Epoch 19/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1729 - accuracy: 0.9349 - val_loss: 0.1826 - val_accuracy: 0.9338\n",
      "Epoch 20/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1712 - accuracy: 0.9346 - val_loss: 0.1770 - val_accuracy: 0.9370\n",
      "Epoch 21/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1655 - accuracy: 0.9380 - val_loss: 0.1890 - val_accuracy: 0.9318\n",
      "Epoch 22/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1622 - accuracy: 0.9385 - val_loss: 0.1803 - val_accuracy: 0.9342\n",
      "Epoch 23/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1619 - accuracy: 0.9376 - val_loss: 0.1801 - val_accuracy: 0.9336\n",
      "Epoch 24/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1585 - accuracy: 0.9411 - val_loss: 0.1872 - val_accuracy: 0.9330\n",
      "Epoch 25/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1545 - accuracy: 0.9415 - val_loss: 0.1864 - val_accuracy: 0.9358\n",
      "Epoch 26/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1531 - accuracy: 0.9417 - val_loss: 0.1838 - val_accuracy: 0.9354\n",
      "Epoch 27/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1474 - accuracy: 0.9440 - val_loss: 0.1798 - val_accuracy: 0.9372\n",
      "Epoch 28/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1489 - accuracy: 0.9431 - val_loss: 0.1864 - val_accuracy: 0.9346\n",
      "Epoch 29/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1463 - accuracy: 0.9438 - val_loss: 0.1891 - val_accuracy: 0.9326\n",
      "Epoch 30/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1434 - accuracy: 0.9453 - val_loss: 0.1887 - val_accuracy: 0.9332\n",
      "Epoch 31/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1394 - accuracy: 0.9472 - val_loss: 0.1877 - val_accuracy: 0.9350\n",
      "Epoch 32/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1389 - accuracy: 0.9465 - val_loss: 0.1874 - val_accuracy: 0.9334\n",
      "Epoch 33/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1355 - accuracy: 0.9493 - val_loss: 0.2034 - val_accuracy: 0.9268\n",
      "Epoch 34/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1342 - accuracy: 0.9494 - val_loss: 0.1904 - val_accuracy: 0.9304\n",
      "Epoch 35/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1343 - accuracy: 0.9487 - val_loss: 0.1796 - val_accuracy: 0.9350\n",
      "Epoch 36/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1338 - accuracy: 0.9491 - val_loss: 0.1923 - val_accuracy: 0.9344\n",
      "Epoch 37/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1309 - accuracy: 0.9508 - val_loss: 0.1919 - val_accuracy: 0.9332\n",
      "Epoch 38/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1328 - accuracy: 0.9491 - val_loss: 0.1860 - val_accuracy: 0.9380\n",
      "Epoch 39/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1301 - accuracy: 0.9499 - val_loss: 0.1879 - val_accuracy: 0.9358\n",
      "Epoch 40/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1285 - accuracy: 0.9509 - val_loss: 0.1861 - val_accuracy: 0.9406\n",
      "Epoch 41/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1289 - accuracy: 0.9509 - val_loss: 0.1893 - val_accuracy: 0.9402\n",
      "Epoch 42/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1283 - accuracy: 0.9521 - val_loss: 0.1899 - val_accuracy: 0.9378\n",
      "Epoch 43/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1241 - accuracy: 0.9523 - val_loss: 0.1939 - val_accuracy: 0.9376\n",
      "Epoch 44/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1251 - accuracy: 0.9518 - val_loss: 0.2021 - val_accuracy: 0.9332\n",
      "Epoch 45/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1250 - accuracy: 0.9525 - val_loss: 0.1895 - val_accuracy: 0.9386\n",
      "Epoch 46/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1202 - accuracy: 0.9537 - val_loss: 0.1954 - val_accuracy: 0.9376\n",
      "Epoch 47/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1233 - accuracy: 0.9530 - val_loss: 0.2009 - val_accuracy: 0.9336\n",
      "Epoch 48/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1198 - accuracy: 0.9540 - val_loss: 0.1955 - val_accuracy: 0.9338\n",
      "Epoch 49/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1175 - accuracy: 0.9557 - val_loss: 0.1900 - val_accuracy: 0.9370\n",
      "Epoch 50/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1188 - accuracy: 0.9547 - val_loss: 0.1985 - val_accuracy: 0.9366\n",
      "Epoch 51/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1171 - accuracy: 0.9555 - val_loss: 0.1995 - val_accuracy: 0.9376\n",
      "Epoch 52/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1181 - accuracy: 0.9541 - val_loss: 0.1929 - val_accuracy: 0.9404\n",
      "Epoch 53/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1162 - accuracy: 0.9556 - val_loss: 0.1952 - val_accuracy: 0.9352\n",
      "Epoch 54/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1150 - accuracy: 0.9568 - val_loss: 0.2143 - val_accuracy: 0.9322\n",
      "Epoch 55/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1124 - accuracy: 0.9575 - val_loss: 0.1937 - val_accuracy: 0.9396\n",
      "Epoch 56/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1142 - accuracy: 0.9560 - val_loss: 0.1937 - val_accuracy: 0.9390\n",
      "Epoch 57/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1121 - accuracy: 0.9566 - val_loss: 0.2091 - val_accuracy: 0.9324\n",
      "Epoch 58/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1134 - accuracy: 0.9575 - val_loss: 0.2009 - val_accuracy: 0.9384\n",
      "Epoch 59/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1108 - accuracy: 0.9577 - val_loss: 0.1949 - val_accuracy: 0.9362\n",
      "Epoch 60/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1100 - accuracy: 0.9593 - val_loss: 0.1996 - val_accuracy: 0.9384\n",
      "Epoch 61/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1080 - accuracy: 0.9589 - val_loss: 0.1939 - val_accuracy: 0.9402\n",
      "Epoch 62/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1090 - accuracy: 0.9578 - val_loss: 0.1986 - val_accuracy: 0.9370\n",
      "Epoch 63/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1122 - accuracy: 0.9577 - val_loss: 0.1976 - val_accuracy: 0.9366\n",
      "Epoch 64/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1103 - accuracy: 0.9579 - val_loss: 0.1920 - val_accuracy: 0.9366\n",
      "Epoch 65/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1054 - accuracy: 0.9595 - val_loss: 0.2041 - val_accuracy: 0.9342\n",
      "Epoch 66/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1053 - accuracy: 0.9605 - val_loss: 0.2025 - val_accuracy: 0.9396\n",
      "Epoch 67/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1052 - accuracy: 0.9599 - val_loss: 0.1985 - val_accuracy: 0.9388\n",
      "Epoch 68/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1070 - accuracy: 0.9600 - val_loss: 0.2044 - val_accuracy: 0.9372\n",
      "Epoch 69/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1057 - accuracy: 0.9590 - val_loss: 0.2012 - val_accuracy: 0.9404\n",
      "Epoch 70/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1077 - accuracy: 0.9609 - val_loss: 0.1960 - val_accuracy: 0.9408\n",
      "Epoch 71/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1080 - accuracy: 0.9596 - val_loss: 0.2008 - val_accuracy: 0.9382\n",
      "Epoch 72/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1059 - accuracy: 0.9598 - val_loss: 0.2125 - val_accuracy: 0.9350\n",
      "Epoch 73/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1035 - accuracy: 0.9611 - val_loss: 0.2009 - val_accuracy: 0.9360\n",
      "Epoch 74/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1059 - accuracy: 0.9603 - val_loss: 0.2053 - val_accuracy: 0.9366\n",
      "Epoch 75/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1051 - accuracy: 0.9611 - val_loss: 0.2076 - val_accuracy: 0.9332\n",
      "Epoch 76/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1061 - accuracy: 0.9599 - val_loss: 0.1970 - val_accuracy: 0.9364\n",
      "Epoch 77/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1034 - accuracy: 0.9617 - val_loss: 0.2079 - val_accuracy: 0.9352\n",
      "Epoch 78/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1010 - accuracy: 0.9622 - val_loss: 0.2060 - val_accuracy: 0.9372\n",
      "Epoch 79/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1037 - accuracy: 0.9611 - val_loss: 0.2106 - val_accuracy: 0.9366\n",
      "Epoch 80/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0998 - accuracy: 0.9623 - val_loss: 0.2047 - val_accuracy: 0.9392\n",
      "Epoch 81/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0966 - accuracy: 0.9634 - val_loss: 0.2090 - val_accuracy: 0.9370\n",
      "Epoch 82/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0998 - accuracy: 0.9616 - val_loss: 0.2115 - val_accuracy: 0.9408\n",
      "Epoch 83/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1002 - accuracy: 0.9627 - val_loss: 0.2092 - val_accuracy: 0.9374\n",
      "Epoch 84/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1002 - accuracy: 0.9614 - val_loss: 0.2175 - val_accuracy: 0.9384\n",
      "Epoch 85/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1011 - accuracy: 0.9627 - val_loss: 0.2062 - val_accuracy: 0.9366\n",
      "Epoch 86/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0953 - accuracy: 0.9641 - val_loss: 0.2062 - val_accuracy: 0.9418\n",
      "Epoch 87/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0983 - accuracy: 0.9628 - val_loss: 0.2131 - val_accuracy: 0.9364\n",
      "Epoch 88/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0976 - accuracy: 0.9631 - val_loss: 0.2134 - val_accuracy: 0.9396\n",
      "Epoch 89/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0937 - accuracy: 0.9650 - val_loss: 0.2213 - val_accuracy: 0.9394\n",
      "Epoch 90/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.1003 - accuracy: 0.9621 - val_loss: 0.2135 - val_accuracy: 0.9336\n",
      "Epoch 91/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0962 - accuracy: 0.9628 - val_loss: 0.2311 - val_accuracy: 0.9324\n",
      "Epoch 92/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0981 - accuracy: 0.9637 - val_loss: 0.2057 - val_accuracy: 0.9386\n",
      "Epoch 93/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0937 - accuracy: 0.9653 - val_loss: 0.2070 - val_accuracy: 0.9412\n",
      "Epoch 94/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0953 - accuracy: 0.9645 - val_loss: 0.2144 - val_accuracy: 0.9390\n",
      "Epoch 95/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0973 - accuracy: 0.9639 - val_loss: 0.2053 - val_accuracy: 0.9386\n",
      "Epoch 96/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0959 - accuracy: 0.9646 - val_loss: 0.2131 - val_accuracy: 0.9340\n",
      "Epoch 97/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0935 - accuracy: 0.9650 - val_loss: 0.2137 - val_accuracy: 0.9368\n",
      "Epoch 98/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0974 - accuracy: 0.9639 - val_loss: 0.2194 - val_accuracy: 0.9384\n",
      "Epoch 99/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0907 - accuracy: 0.9662 - val_loss: 0.2136 - val_accuracy: 0.9350\n",
      "Epoch 100/100\n",
      "430/430 [==============================] - 2s 5ms/step - loss: 0.0917 - accuracy: 0.9656 - val_loss: 0.2139 - val_accuracy: 0.9388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9816050d30>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='Adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=128,\n",
    "         epochs=100,\n",
    "         validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "T34mruAp5F5K",
    "outputId": "8e56667b-01a6-4c30-e334-55e0b866ea5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 93.05999875068665\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbhr44DnMbKZ"
   },
   "source": [
    "\n",
    "### Task 1.3 Build an input pipeline for data augmentation\n",
    "\n",
    "*(weight ~5%)*\n",
    "\n",
    "Build a data preprocessing pipeline to perform data augmentation. (You may use Keras ImageDataGenerator or write your own transformations.)\n",
    "\n",
    "- Report the new classification accuracy. Make sure that you use the same number of training epochs as in Task 1.2.\n",
    "\n",
    "- (Optional) Profile your input pipeline to identify the most time-consuming operation. What actions have you taken to address that slow operation? (*Hint: You may use the [TensorFlow Profiler](https://github.com/tensorflow/profiler).*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_w0ppH0HFyX"
   },
   "outputs": [],
   "source": [
    "#Building data preprocessing pipeline top perform data augmentation\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.5,\n",
    "        zoom_range=(0.9, 1.1),\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False, \n",
    "        fill_mode='constant',\n",
    "        cval=0)\n",
    "    \n",
    "val_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgJXDZP6Hbky"
   },
   "outputs": [],
   "source": [
    "train_datagen.fit(x_train)\n",
    "#val_datagen.fit(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdV-oqp8JU13"
   },
   "outputs": [],
   "source": [
    "num_train_batches = len(x_train) // batch_size\n",
    "num_train_batches += (0 if len(x_train) % batch_size == 0 else 1)     \n",
    "num_val_batches = len(x_valid) // batch_size\n",
    "num_val_batches += (0 if len(x_valid) % batch_size == 0 else 1)     \n",
    "num_test_batches = len(x_test) // batch_size\n",
    "num_test_batches += (0 if len(x_test) % batch_size == 0 else 1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ghu1aODtJj6n"
   },
   "outputs": [],
   "source": [
    "#pipelines for training, validation and test data\n",
    "\n",
    "train_generator = train_datagen.flow(x_train, y_train, \n",
    "                                         batch_size=batch_size)\n",
    "\n",
    "val_generator = train_datagen.flow(x_valid, y_valid, \n",
    "                                         batch_size=batch_size)\n",
    "\n",
    "test_generator = test_datagen.flow(x_test, y_test, \n",
    "                                       batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Eo692HCoQX-r",
    "outputId": "230903b9-53d8-4c44-eed8-6a420cf92de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.5777 - accuracy: 0.7915 - val_loss: 0.4295 - val_accuracy: 0.8418\n",
      "Epoch 3/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.5258 - accuracy: 0.8040 - val_loss: 0.4342 - val_accuracy: 0.8458\n",
      "Epoch 4/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.5245 - accuracy: 0.8099 - val_loss: 0.3819 - val_accuracy: 0.8684\n",
      "Epoch 5/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.4905 - accuracy: 0.8247 - val_loss: 0.3774 - val_accuracy: 0.8634\n",
      "Epoch 6/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.4733 - accuracy: 0.8277 - val_loss: 0.3790 - val_accuracy: 0.8592\n",
      "Epoch 7/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.4517 - accuracy: 0.8330 - val_loss: 0.3486 - val_accuracy: 0.8682\n",
      "Epoch 8/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.4619 - accuracy: 0.8313 - val_loss: 0.3651 - val_accuracy: 0.8650\n",
      "Epoch 9/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.4505 - accuracy: 0.8317 - val_loss: 0.3483 - val_accuracy: 0.8704\n",
      "Epoch 10/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.4543 - accuracy: 0.8335 - val_loss: 0.3509 - val_accuracy: 0.8676\n",
      "Epoch 11/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.4380 - accuracy: 0.8383 - val_loss: 0.3541 - val_accuracy: 0.8598\n",
      "Epoch 12/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.4361 - accuracy: 0.8401 - val_loss: 0.3416 - val_accuracy: 0.8726\n",
      "Epoch 13/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.4290 - accuracy: 0.8429 - val_loss: 0.3308 - val_accuracy: 0.8760\n",
      "Epoch 14/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.4235 - accuracy: 0.8425 - val_loss: 0.3287 - val_accuracy: 0.8826\n",
      "Epoch 15/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.4207 - accuracy: 0.8449 - val_loss: 0.3210 - val_accuracy: 0.8782\n",
      "Epoch 16/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.4057 - accuracy: 0.8508 - val_loss: 0.3226 - val_accuracy: 0.8780\n",
      "Epoch 17/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.4119 - accuracy: 0.8478 - val_loss: 0.3247 - val_accuracy: 0.8780\n",
      "Epoch 18/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.4051 - accuracy: 0.8516 - val_loss: 0.3269 - val_accuracy: 0.8720\n",
      "Epoch 19/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3933 - accuracy: 0.8537 - val_loss: 0.3151 - val_accuracy: 0.8848\n",
      "Epoch 20/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.4017 - accuracy: 0.8567 - val_loss: 0.3065 - val_accuracy: 0.8838\n",
      "Epoch 21/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3907 - accuracy: 0.8542 - val_loss: 0.3039 - val_accuracy: 0.8866\n",
      "Epoch 22/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3968 - accuracy: 0.8529 - val_loss: 0.3252 - val_accuracy: 0.8804\n",
      "Epoch 23/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3930 - accuracy: 0.8521 - val_loss: 0.2950 - val_accuracy: 0.8906\n",
      "Epoch 24/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3797 - accuracy: 0.8567 - val_loss: 0.2883 - val_accuracy: 0.8896\n",
      "Epoch 25/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3790 - accuracy: 0.8555 - val_loss: 0.3006 - val_accuracy: 0.8866\n",
      "Epoch 26/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3952 - accuracy: 0.8549 - val_loss: 0.3100 - val_accuracy: 0.8842\n",
      "Epoch 27/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3856 - accuracy: 0.8622 - val_loss: 0.3004 - val_accuracy: 0.8882\n",
      "Epoch 28/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3838 - accuracy: 0.8608 - val_loss: 0.2922 - val_accuracy: 0.8916\n",
      "Epoch 29/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3763 - accuracy: 0.8650 - val_loss: 0.2956 - val_accuracy: 0.8920\n",
      "Epoch 30/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3587 - accuracy: 0.8678 - val_loss: 0.3084 - val_accuracy: 0.8852\n",
      "Epoch 31/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3744 - accuracy: 0.8590 - val_loss: 0.2945 - val_accuracy: 0.8868\n",
      "Epoch 32/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3725 - accuracy: 0.8641 - val_loss: 0.2852 - val_accuracy: 0.8966\n",
      "Epoch 33/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3653 - accuracy: 0.8663 - val_loss: 0.2873 - val_accuracy: 0.8932\n",
      "Epoch 34/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3509 - accuracy: 0.8684 - val_loss: 0.2823 - val_accuracy: 0.8964\n",
      "Epoch 35/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3509 - accuracy: 0.8694 - val_loss: 0.2895 - val_accuracy: 0.8854\n",
      "Epoch 36/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3608 - accuracy: 0.8687 - val_loss: 0.2764 - val_accuracy: 0.8992\n",
      "Epoch 37/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3665 - accuracy: 0.8629 - val_loss: 0.2983 - val_accuracy: 0.8882\n",
      "Epoch 38/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3707 - accuracy: 0.8636 - val_loss: 0.2904 - val_accuracy: 0.8948\n",
      "Epoch 39/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3667 - accuracy: 0.8654 - val_loss: 0.2919 - val_accuracy: 0.8896\n",
      "Epoch 40/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3716 - accuracy: 0.8578 - val_loss: 0.2837 - val_accuracy: 0.8956\n",
      "Epoch 41/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3469 - accuracy: 0.8672 - val_loss: 0.2763 - val_accuracy: 0.8962\n",
      "Epoch 42/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3551 - accuracy: 0.8688 - val_loss: 0.3106 - val_accuracy: 0.8804\n",
      "Epoch 43/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3489 - accuracy: 0.8704 - val_loss: 0.2739 - val_accuracy: 0.8976\n",
      "Epoch 44/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3552 - accuracy: 0.8693 - val_loss: 0.2870 - val_accuracy: 0.8892\n",
      "Epoch 45/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3468 - accuracy: 0.8717 - val_loss: 0.2715 - val_accuracy: 0.8992\n",
      "Epoch 46/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3518 - accuracy: 0.8687 - val_loss: 0.2717 - val_accuracy: 0.8970\n",
      "Epoch 47/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3652 - accuracy: 0.8642 - val_loss: 0.2658 - val_accuracy: 0.9012\n",
      "Epoch 48/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3533 - accuracy: 0.8694 - val_loss: 0.2879 - val_accuracy: 0.8920\n",
      "Epoch 49/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3349 - accuracy: 0.8721 - val_loss: 0.2736 - val_accuracy: 0.9004\n",
      "Epoch 50/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3563 - accuracy: 0.8699 - val_loss: 0.2823 - val_accuracy: 0.9002\n",
      "Epoch 51/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3617 - accuracy: 0.8647 - val_loss: 0.2728 - val_accuracy: 0.8990\n",
      "Epoch 52/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3597 - accuracy: 0.8683 - val_loss: 0.2846 - val_accuracy: 0.8968\n",
      "Epoch 53/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3378 - accuracy: 0.8733 - val_loss: 0.2751 - val_accuracy: 0.8952\n",
      "Epoch 54/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3608 - accuracy: 0.8657 - val_loss: 0.2932 - val_accuracy: 0.8914\n",
      "Epoch 55/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3487 - accuracy: 0.8691 - val_loss: 0.2724 - val_accuracy: 0.8994\n",
      "Epoch 56/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3345 - accuracy: 0.8760 - val_loss: 0.2682 - val_accuracy: 0.9012\n",
      "Epoch 57/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3448 - accuracy: 0.8739 - val_loss: 0.2620 - val_accuracy: 0.9028\n",
      "Epoch 58/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3323 - accuracy: 0.8774 - val_loss: 0.2652 - val_accuracy: 0.8996\n",
      "Epoch 59/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3529 - accuracy: 0.8657 - val_loss: 0.2712 - val_accuracy: 0.8960\n",
      "Epoch 60/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3444 - accuracy: 0.8687 - val_loss: 0.2632 - val_accuracy: 0.9032\n",
      "Epoch 61/100\n",
      "79/79 [==============================] - 3s 44ms/step - loss: 0.3395 - accuracy: 0.8767 - val_loss: 0.2637 - val_accuracy: 0.9026\n",
      "Epoch 62/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3288 - accuracy: 0.8794 - val_loss: 0.2751 - val_accuracy: 0.9002\n",
      "Epoch 63/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3319 - accuracy: 0.8769 - val_loss: 0.2761 - val_accuracy: 0.8940\n",
      "Epoch 64/100\n",
      "79/79 [==============================] - 3s 41ms/step - loss: 0.3386 - accuracy: 0.8733 - val_loss: 0.2568 - val_accuracy: 0.9036\n",
      "Epoch 65/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3321 - accuracy: 0.8784 - val_loss: 0.2726 - val_accuracy: 0.8960\n",
      "Epoch 66/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3212 - accuracy: 0.8792 - val_loss: 0.2764 - val_accuracy: 0.8928\n",
      "Epoch 67/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3373 - accuracy: 0.8748 - val_loss: 0.2607 - val_accuracy: 0.9048\n",
      "Epoch 68/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3375 - accuracy: 0.8773 - val_loss: 0.2700 - val_accuracy: 0.9026\n",
      "Epoch 69/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3454 - accuracy: 0.8732 - val_loss: 0.2695 - val_accuracy: 0.9050\n",
      "Epoch 70/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3351 - accuracy: 0.8778 - val_loss: 0.2643 - val_accuracy: 0.9000\n",
      "Epoch 71/100\n",
      "79/79 [==============================] - 3s 41ms/step - loss: 0.3294 - accuracy: 0.8759 - val_loss: 0.2652 - val_accuracy: 0.8976\n",
      "Epoch 72/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3253 - accuracy: 0.8803 - val_loss: 0.2612 - val_accuracy: 0.9006\n",
      "Epoch 73/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3308 - accuracy: 0.8749 - val_loss: 0.2667 - val_accuracy: 0.9014\n",
      "Epoch 74/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3453 - accuracy: 0.8728 - val_loss: 0.2514 - val_accuracy: 0.9052\n",
      "Epoch 75/100\n",
      "79/79 [==============================] - 3s 41ms/step - loss: 0.3443 - accuracy: 0.8720 - val_loss: 0.2598 - val_accuracy: 0.9012\n",
      "Epoch 76/100\n",
      "79/79 [==============================] - 3s 41ms/step - loss: 0.3272 - accuracy: 0.8841 - val_loss: 0.2956 - val_accuracy: 0.8956\n",
      "Epoch 77/100\n",
      "79/79 [==============================] - 3s 41ms/step - loss: 0.3252 - accuracy: 0.8774 - val_loss: 0.2673 - val_accuracy: 0.9010\n",
      "Epoch 78/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3222 - accuracy: 0.8824 - val_loss: 0.2511 - val_accuracy: 0.9020\n",
      "Epoch 79/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3219 - accuracy: 0.8791 - val_loss: 0.2572 - val_accuracy: 0.9052\n",
      "Epoch 80/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3342 - accuracy: 0.8752 - val_loss: 0.2702 - val_accuracy: 0.9008\n",
      "Epoch 81/100\n",
      "79/79 [==============================] - 3s 41ms/step - loss: 0.3277 - accuracy: 0.8775 - val_loss: 0.2848 - val_accuracy: 0.8896\n",
      "Epoch 82/100\n",
      "79/79 [==============================] - 3s 41ms/step - loss: 0.3238 - accuracy: 0.8814 - val_loss: 0.2756 - val_accuracy: 0.8966\n",
      "Epoch 83/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3246 - accuracy: 0.8790 - val_loss: 0.2556 - val_accuracy: 0.9006\n",
      "Epoch 84/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3365 - accuracy: 0.8765 - val_loss: 0.2534 - val_accuracy: 0.9062\n",
      "Epoch 85/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3277 - accuracy: 0.8795 - val_loss: 0.2566 - val_accuracy: 0.9028\n",
      "Epoch 86/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3335 - accuracy: 0.8821 - val_loss: 0.2623 - val_accuracy: 0.9006\n",
      "Epoch 87/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3403 - accuracy: 0.8734 - val_loss: 0.2530 - val_accuracy: 0.9054\n",
      "Epoch 88/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3331 - accuracy: 0.8756 - val_loss: 0.2581 - val_accuracy: 0.9056\n",
      "Epoch 89/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3150 - accuracy: 0.8823 - val_loss: 0.2664 - val_accuracy: 0.8938\n",
      "Epoch 90/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3129 - accuracy: 0.8814 - val_loss: 0.2530 - val_accuracy: 0.9052\n",
      "Epoch 91/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3313 - accuracy: 0.8746 - val_loss: 0.2565 - val_accuracy: 0.9046\n",
      "Epoch 92/100\n",
      "79/79 [==============================] - 3s 44ms/step - loss: 0.3441 - accuracy: 0.8712 - val_loss: 0.2613 - val_accuracy: 0.8998\n",
      "Epoch 93/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3202 - accuracy: 0.8814 - val_loss: 0.2591 - val_accuracy: 0.9032\n",
      "Epoch 94/100\n",
      "79/79 [==============================] - 3s 43ms/step - loss: 0.3291 - accuracy: 0.8792 - val_loss: 0.2512 - val_accuracy: 0.9058\n",
      "Epoch 95/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3381 - accuracy: 0.8703 - val_loss: 0.2591 - val_accuracy: 0.9046\n",
      "Epoch 96/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3111 - accuracy: 0.8824 - val_loss: 0.2542 - val_accuracy: 0.9014\n",
      "Epoch 97/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3177 - accuracy: 0.8822 - val_loss: 0.2558 - val_accuracy: 0.9104\n",
      "Epoch 98/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3173 - accuracy: 0.8807 - val_loss: 0.2585 - val_accuracy: 0.9042\n",
      "Epoch 99/100\n",
      "79/79 [==============================] - 3s 41ms/step - loss: 0.3115 - accuracy: 0.8871 - val_loss: 0.2649 - val_accuracy: 0.8960\n",
      "Epoch 100/100\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 0.3052 - accuracy: 0.8861 - val_loss: 0.2493 - val_accuracy: 0.9008\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model after Data Augmentation\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                                  steps_per_epoch = num_test_batches, \n",
    "                                  epochs = 100,\n",
    "                                  validation_data = val_generator,\n",
    "                                  shuffle=True,verbose = 1,validation_steps = num_val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "fkOMsNlRQYFI",
    "outputId": "d26ab332-ddd7-446a-a2e6-cd0e5257c8c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 91.75000190734863\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prctXU4BswKK"
   },
   "source": [
    "### Task 1.3 Fashion-MNIST with transfer learning\n",
    "\n",
    "*(weight ~5%)*\n",
    "\n",
    "Use a pretrained model as the convolutional base to improve the classification performance. (Hint: You may use models in Keras Applications or those in the TensorFlow Hub.)\n",
    "\n",
    "- Try both with fine-tuning and without fine-tuning.\n",
    "- Report the model performance as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WpspQGOltALM"
   },
   "source": [
    "Note : I would be using the VGG16 pretrained model to classify the **fashion_mnist** dataser and then check it's accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yl9yn458fdGx"
   },
   "outputs": [],
   "source": [
    "#I would be using the VGG16 pretrained model\n",
    "#Have to load the data again as the preprocessing woiuld be different for the VGG19 model\n",
    "#Taken help from https://www.kaggle.com/anandad/classify-fashion-mnist-with-vgg16\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "IMG_WIDTH = 48\n",
    "IMG_HEIGHT = 48\n",
    "IMG_DEPTH = 3\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "Z8isJoQXsXBk",
    "outputId": "598a13c9-90f4-49f8-91c0-217d553fcf2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 84), (10000, 28, 84))"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We had to convert the images into 3 channels\n",
    "\n",
    "train_X=np.dstack([x_train] * 3)\n",
    "test_X=np.dstack([x_test]*3)\n",
    "train_X.shape,test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "RGOie2qDPVSh",
    "outputId": "dac58487-b73e-40a4-eb5c-e28325905ece"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 3), (10000, 28, 28, 3))"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reshaping the images as the input required by the tensorflow\n",
    "\n",
    "train_X = train_X.reshape(-1, 28,28,3)\n",
    "test_X= test_X.reshape (-1,28,28,3)\n",
    "train_X.shape,test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "eBGWJryVPVfw",
    "outputId": "38b94d7b-c5e7-46e8-b62c-a1484d5029f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 48, 48, 3), (10000, 48, 48, 3))"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VGG19 requires the size of the image to be 48*48\n",
    "\n",
    "train_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in train_X])\n",
    "test_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in test_X])\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MugVgziPVtZ"
   },
   "outputs": [],
   "source": [
    "#Normalising the data and changing their type to float\n",
    "\n",
    "train_X = train_X / 255.\n",
    "test_X = test_X / 255.\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "jzigjiG6PV73",
    "outputId": "e0207043-c350-47aa-e976-85bf7473526a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 48, 48, 3), (12000, 48, 48, 3), (48000, 10), (12000, 10))"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Labels to one hot encoded format\n",
    "\n",
    "train_Y_one_hot = to_categorical(y_train)\n",
    "test_Y_one_hot = to_categorical(y_test)\n",
    "\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X,\n",
    "                                                           train_Y_one_hot,\n",
    "                                                           test_size=0.2,\n",
    "                                                           random_state=13\n",
    "                                                           )\n",
    "train_X.shape,valid_X.shape,train_label.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxjO9gwKfdfb"
   },
   "outputs": [],
   "source": [
    "#Preprocessing the inputs\n",
    "\n",
    "train_X = preprocess_input(train_X)\n",
    "valid_X = preprocess_input(valid_X)\n",
    "test_X  = preprocess_input (test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "colab_type": "code",
    "id": "eCJwauvvfdxf",
    "outputId": "2fcc9cba-258d-4831-b8aa-48afe9f49a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 48, 48, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 48, 48, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False, \n",
    "                  input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH)\n",
    "                 )\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "uesPhaxEowh0",
    "outputId": "1a6a2db5-24c1-4a0c-e1c2-36d15d5d6b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 19s 6ms/step\n",
      "625/625 [==============================] - 4s 6ms/step\n",
      "750/750 [==============================] - 5s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extracting features\n",
    "\n",
    "train_features = conv_base.predict(np.array(train_X), batch_size=BATCH_SIZE, verbose=1)\n",
    "test_features = conv_base.predict(np.array(test_X), batch_size=BATCH_SIZE, verbose=1)\n",
    "val_features = conv_base.predict(np.array(valid_X), batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2MuBCTS8RCdp"
   },
   "outputs": [],
   "source": [
    "# Flatten extracted features\n",
    "\n",
    "train_features_flat = np.reshape(train_features, (48000, 1*1*512))\n",
    "test_features_flat = np.reshape(test_features, (10000, 1*1*512))\n",
    "val_features_flat = np.reshape(val_features, (12000, 1*1*512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G83Yrl-Vp2WK"
   },
   "outputs": [],
   "source": [
    "#Making the model\n",
    "\n",
    "model = models.Sequential()\n",
    "#model.add(conv_base)\n",
    "model.add(layers.Dense(512, activation='relu', input_dim=1 * 1 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Hdsw7C6up2g2",
    "outputId": "f45fc06b-6f12-49b2-905e-80f4206a0eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.7537 - accuracy: 0.3802 - val_loss: 1.2591 - val_accuracy: 0.5663\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.2690 - accuracy: 0.5300 - val_loss: 1.1060 - val_accuracy: 0.6072\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.1720 - accuracy: 0.5652 - val_loss: 1.0376 - val_accuracy: 0.6385\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.1172 - accuracy: 0.5879 - val_loss: 0.9699 - val_accuracy: 0.6571\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0780 - accuracy: 0.6009 - val_loss: 0.9497 - val_accuracy: 0.6576\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0647 - accuracy: 0.6074 - val_loss: 0.9350 - val_accuracy: 0.6754\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0630 - accuracy: 0.6048 - val_loss: 0.9640 - val_accuracy: 0.6461\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0558 - accuracy: 0.6108 - val_loss: 0.8998 - val_accuracy: 0.6660\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0355 - accuracy: 0.6174 - val_loss: 0.9130 - val_accuracy: 0.6604\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0171 - accuracy: 0.6227 - val_loss: 0.8991 - val_accuracy: 0.6628\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0135 - accuracy: 0.6256 - val_loss: 0.8751 - val_accuracy: 0.6973\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0128 - accuracy: 0.6230 - val_loss: 0.8794 - val_accuracy: 0.6801\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0105 - accuracy: 0.6238 - val_loss: 0.8871 - val_accuracy: 0.6748\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0064 - accuracy: 0.6288 - val_loss: 0.8696 - val_accuracy: 0.6795\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0108 - accuracy: 0.6271 - val_loss: 0.8511 - val_accuracy: 0.6787\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0065 - accuracy: 0.6261 - val_loss: 0.8717 - val_accuracy: 0.6952\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9911 - accuracy: 0.6350 - val_loss: 0.8216 - val_accuracy: 0.7064\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9896 - accuracy: 0.6369 - val_loss: 0.8992 - val_accuracy: 0.6643\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.0001 - accuracy: 0.6314 - val_loss: 0.8379 - val_accuracy: 0.7053\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9902 - accuracy: 0.6337 - val_loss: 0.8624 - val_accuracy: 0.6827\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9871 - accuracy: 0.6347 - val_loss: 0.8298 - val_accuracy: 0.7026\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9901 - accuracy: 0.6342 - val_loss: 0.8728 - val_accuracy: 0.6839\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9994 - accuracy: 0.6298 - val_loss: 0.8294 - val_accuracy: 0.7048\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9935 - accuracy: 0.6351 - val_loss: 0.8229 - val_accuracy: 0.6969\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9981 - accuracy: 0.6341 - val_loss: 0.8321 - val_accuracy: 0.7052\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9856 - accuracy: 0.6367 - val_loss: 0.8400 - val_accuracy: 0.7018\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9867 - accuracy: 0.6390 - val_loss: 0.8203 - val_accuracy: 0.7060\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9812 - accuracy: 0.6409 - val_loss: 0.8545 - val_accuracy: 0.7054\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9817 - accuracy: 0.6397 - val_loss: 0.8240 - val_accuracy: 0.7149\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9757 - accuracy: 0.6423 - val_loss: 0.8504 - val_accuracy: 0.6845\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9685 - accuracy: 0.6419 - val_loss: 0.8098 - val_accuracy: 0.7159\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9759 - accuracy: 0.6426 - val_loss: 0.8485 - val_accuracy: 0.6748\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9772 - accuracy: 0.6385 - val_loss: 0.8083 - val_accuracy: 0.7163\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9781 - accuracy: 0.6386 - val_loss: 0.8103 - val_accuracy: 0.7025\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9688 - accuracy: 0.6441 - val_loss: 0.8528 - val_accuracy: 0.6988\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9749 - accuracy: 0.6408 - val_loss: 0.8144 - val_accuracy: 0.7122\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9634 - accuracy: 0.6477 - val_loss: 0.8139 - val_accuracy: 0.7153\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9642 - accuracy: 0.6462 - val_loss: 0.8266 - val_accuracy: 0.7125\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9536 - accuracy: 0.6484 - val_loss: 0.7917 - val_accuracy: 0.7191\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9552 - accuracy: 0.6476 - val_loss: 0.8194 - val_accuracy: 0.7002\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9629 - accuracy: 0.6466 - val_loss: 0.8479 - val_accuracy: 0.6851\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9574 - accuracy: 0.6479 - val_loss: 0.8078 - val_accuracy: 0.7133\n",
      "Epoch 43/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9474 - accuracy: 0.6515 - val_loss: 0.7892 - val_accuracy: 0.7218\n",
      "Epoch 44/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9507 - accuracy: 0.6511 - val_loss: 0.8129 - val_accuracy: 0.7122\n",
      "Epoch 45/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9501 - accuracy: 0.6509 - val_loss: 0.8035 - val_accuracy: 0.7170\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9518 - accuracy: 0.6496 - val_loss: 0.7816 - val_accuracy: 0.7190\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9474 - accuracy: 0.6528 - val_loss: 0.7910 - val_accuracy: 0.7064\n",
      "Epoch 48/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9368 - accuracy: 0.6553 - val_loss: 0.7797 - val_accuracy: 0.7195\n",
      "Epoch 49/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9464 - accuracy: 0.6541 - val_loss: 0.8002 - val_accuracy: 0.7122\n",
      "Epoch 50/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9445 - accuracy: 0.6534 - val_loss: 0.8149 - val_accuracy: 0.7087\n",
      "Epoch 51/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9497 - accuracy: 0.6510 - val_loss: 0.7997 - val_accuracy: 0.7114\n",
      "Epoch 52/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9435 - accuracy: 0.6542 - val_loss: 0.8046 - val_accuracy: 0.7114\n",
      "Epoch 53/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9430 - accuracy: 0.6546 - val_loss: 0.8042 - val_accuracy: 0.7113\n",
      "Epoch 54/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9366 - accuracy: 0.6560 - val_loss: 0.8035 - val_accuracy: 0.7155\n",
      "Epoch 55/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9401 - accuracy: 0.6567 - val_loss: 0.7979 - val_accuracy: 0.7090\n",
      "Epoch 56/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9434 - accuracy: 0.6515 - val_loss: 0.8325 - val_accuracy: 0.6998\n",
      "Epoch 57/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9457 - accuracy: 0.6517 - val_loss: 0.7796 - val_accuracy: 0.7252\n",
      "Epoch 58/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9426 - accuracy: 0.6521 - val_loss: 0.7920 - val_accuracy: 0.7085\n",
      "Epoch 59/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9378 - accuracy: 0.6534 - val_loss: 0.8107 - val_accuracy: 0.7107\n",
      "Epoch 60/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9417 - accuracy: 0.6546 - val_loss: 0.7730 - val_accuracy: 0.7288\n",
      "Epoch 61/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9464 - accuracy: 0.6498 - val_loss: 0.7915 - val_accuracy: 0.7234\n",
      "Epoch 62/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9461 - accuracy: 0.6529 - val_loss: 0.7774 - val_accuracy: 0.7198\n",
      "Epoch 63/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9447 - accuracy: 0.6525 - val_loss: 0.7955 - val_accuracy: 0.7194\n",
      "Epoch 64/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9412 - accuracy: 0.6536 - val_loss: 0.7871 - val_accuracy: 0.7193\n",
      "Epoch 65/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9352 - accuracy: 0.6620 - val_loss: 0.7629 - val_accuracy: 0.7249\n",
      "Epoch 66/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9377 - accuracy: 0.6536 - val_loss: 0.7942 - val_accuracy: 0.7239\n",
      "Epoch 67/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9391 - accuracy: 0.6578 - val_loss: 0.7815 - val_accuracy: 0.7258\n",
      "Epoch 68/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9283 - accuracy: 0.6623 - val_loss: 0.7862 - val_accuracy: 0.7238\n",
      "Epoch 69/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9343 - accuracy: 0.6574 - val_loss: 0.8108 - val_accuracy: 0.7046\n",
      "Epoch 70/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9340 - accuracy: 0.6586 - val_loss: 0.7967 - val_accuracy: 0.7150\n",
      "Epoch 71/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9394 - accuracy: 0.6561 - val_loss: 0.7908 - val_accuracy: 0.7157\n",
      "Epoch 72/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9276 - accuracy: 0.6589 - val_loss: 0.7897 - val_accuracy: 0.7138\n",
      "Epoch 73/100\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.9348 - accuracy: 0.6574 - val_loss: 0.7653 - val_accuracy: 0.7271\n",
      "Epoch 74/100\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.9386 - accuracy: 0.6568 - val_loss: 0.7705 - val_accuracy: 0.7287\n",
      "Epoch 75/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9336 - accuracy: 0.6573 - val_loss: 0.7734 - val_accuracy: 0.7205\n",
      "Epoch 76/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9324 - accuracy: 0.6563 - val_loss: 0.7738 - val_accuracy: 0.7161\n",
      "Epoch 77/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9271 - accuracy: 0.6618 - val_loss: 0.7618 - val_accuracy: 0.7247\n",
      "Epoch 78/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9300 - accuracy: 0.6586 - val_loss: 0.7901 - val_accuracy: 0.7064\n",
      "Epoch 79/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9300 - accuracy: 0.6577 - val_loss: 0.7781 - val_accuracy: 0.7218\n",
      "Epoch 80/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9268 - accuracy: 0.6598 - val_loss: 0.7582 - val_accuracy: 0.7317\n",
      "Epoch 81/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9295 - accuracy: 0.6585 - val_loss: 0.7749 - val_accuracy: 0.7208\n",
      "Epoch 82/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9303 - accuracy: 0.6581 - val_loss: 0.7722 - val_accuracy: 0.7246\n",
      "Epoch 83/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9248 - accuracy: 0.6627 - val_loss: 0.7785 - val_accuracy: 0.7138\n",
      "Epoch 84/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9235 - accuracy: 0.6616 - val_loss: 0.7760 - val_accuracy: 0.7153\n",
      "Epoch 85/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9248 - accuracy: 0.6643 - val_loss: 0.7674 - val_accuracy: 0.7300\n",
      "Epoch 86/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9241 - accuracy: 0.6611 - val_loss: 0.7642 - val_accuracy: 0.7303\n",
      "Epoch 87/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9233 - accuracy: 0.6634 - val_loss: 0.7729 - val_accuracy: 0.7220\n",
      "Epoch 88/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9204 - accuracy: 0.6631 - val_loss: 0.7660 - val_accuracy: 0.7237\n",
      "Epoch 89/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9219 - accuracy: 0.6620 - val_loss: 0.7625 - val_accuracy: 0.7248\n",
      "Epoch 90/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9230 - accuracy: 0.6656 - val_loss: 0.8121 - val_accuracy: 0.7012\n",
      "Epoch 91/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9166 - accuracy: 0.6628 - val_loss: 0.7900 - val_accuracy: 0.7113\n",
      "Epoch 92/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9199 - accuracy: 0.6633 - val_loss: 0.7678 - val_accuracy: 0.7204\n",
      "Epoch 93/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9220 - accuracy: 0.6639 - val_loss: 0.7704 - val_accuracy: 0.7155\n",
      "Epoch 94/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9228 - accuracy: 0.6629 - val_loss: 0.7619 - val_accuracy: 0.7283\n",
      "Epoch 95/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9128 - accuracy: 0.6702 - val_loss: 0.7899 - val_accuracy: 0.7174\n",
      "Epoch 96/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9179 - accuracy: 0.6663 - val_loss: 0.7566 - val_accuracy: 0.7311\n",
      "Epoch 97/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9139 - accuracy: 0.6662 - val_loss: 0.7605 - val_accuracy: 0.7250\n",
      "Epoch 98/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9193 - accuracy: 0.6633 - val_loss: 0.7593 - val_accuracy: 0.7230\n",
      "Epoch 99/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9152 - accuracy: 0.6673 - val_loss: 0.7683 - val_accuracy: 0.7264\n",
      "Epoch 100/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9223 - accuracy: 0.6624 - val_loss: 0.7534 - val_accuracy: 0.7324\n"
     ]
    }
   ],
   "source": [
    "# Train the the model without FINE TUNING\n",
    "\n",
    "history = model.fit(\n",
    "    train_features_flat,\n",
    "    train_label,\n",
    "    epochs=100,\n",
    "    validation_data=(val_features_flat, valid_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "zji9rNjFrfWe",
    "outputId": "3f3e015f-0639-4953-b847-d66369a127cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 72.3800003528595\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_features_flat,test_Y_one_hot, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-b74Ti9D1V4"
   },
   "outputs": [],
   "source": [
    "#Fine tuning the model\n",
    "\n",
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "# Making the model\n",
    "\n",
    "model = models.Sequential() \n",
    "#model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu')) \n",
    "model.add(layers.Dense(10, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Tn2D7Ag_4kWW",
    "outputId": "d29df4d8-8896-4bdc-a834-0d8baacd9062"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9187 - accuracy: 0.6658 - val_loss: 0.9188 - val_accuracy: 0.6627\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.8783 - accuracy: 0.6787 - val_loss: 0.8203 - val_accuracy: 0.7044\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.8496 - accuracy: 0.6913 - val_loss: 0.8842 - val_accuracy: 0.6780\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.8359 - accuracy: 0.6930 - val_loss: 0.8312 - val_accuracy: 0.6955\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.8175 - accuracy: 0.7031 - val_loss: 0.8326 - val_accuracy: 0.6996\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.8084 - accuracy: 0.7053 - val_loss: 0.7712 - val_accuracy: 0.7221\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7976 - accuracy: 0.7078 - val_loss: 0.7802 - val_accuracy: 0.7142\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7912 - accuracy: 0.7113 - val_loss: 0.8004 - val_accuracy: 0.7103\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7833 - accuracy: 0.7142 - val_loss: 0.8379 - val_accuracy: 0.6836\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7785 - accuracy: 0.7144 - val_loss: 0.8183 - val_accuracy: 0.7079\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7761 - accuracy: 0.7147 - val_loss: 0.7497 - val_accuracy: 0.7321\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7670 - accuracy: 0.7214 - val_loss: 0.7311 - val_accuracy: 0.7358\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7614 - accuracy: 0.7231 - val_loss: 0.7978 - val_accuracy: 0.7162\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7557 - accuracy: 0.7253 - val_loss: 0.8633 - val_accuracy: 0.6646\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7521 - accuracy: 0.7269 - val_loss: 0.7297 - val_accuracy: 0.7431\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7520 - accuracy: 0.7263 - val_loss: 0.7978 - val_accuracy: 0.7110\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7445 - accuracy: 0.7263 - val_loss: 0.8261 - val_accuracy: 0.6816\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7423 - accuracy: 0.7306 - val_loss: 0.7254 - val_accuracy: 0.7304\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7415 - accuracy: 0.7293 - val_loss: 0.7203 - val_accuracy: 0.7410\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7350 - accuracy: 0.7314 - val_loss: 0.7303 - val_accuracy: 0.7333\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7343 - accuracy: 0.7299 - val_loss: 0.7416 - val_accuracy: 0.7335\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7286 - accuracy: 0.7342 - val_loss: 0.7253 - val_accuracy: 0.7339\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7299 - accuracy: 0.7334 - val_loss: 0.7436 - val_accuracy: 0.7363\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7277 - accuracy: 0.7351 - val_loss: 0.7376 - val_accuracy: 0.7353\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7247 - accuracy: 0.7354 - val_loss: 0.7010 - val_accuracy: 0.7450\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7160 - accuracy: 0.7395 - val_loss: 0.7333 - val_accuracy: 0.7406\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7228 - accuracy: 0.7367 - val_loss: 0.6969 - val_accuracy: 0.7531\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7141 - accuracy: 0.7396 - val_loss: 0.7651 - val_accuracy: 0.7268\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7133 - accuracy: 0.7384 - val_loss: 0.7121 - val_accuracy: 0.7452\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7135 - accuracy: 0.7390 - val_loss: 0.8148 - val_accuracy: 0.7005\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7113 - accuracy: 0.7396 - val_loss: 0.6995 - val_accuracy: 0.7508\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7121 - accuracy: 0.7422 - val_loss: 0.7001 - val_accuracy: 0.7509\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7099 - accuracy: 0.7406 - val_loss: 0.7710 - val_accuracy: 0.7108\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7049 - accuracy: 0.7437 - val_loss: 0.7304 - val_accuracy: 0.7288\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7067 - accuracy: 0.7411 - val_loss: 0.7124 - val_accuracy: 0.7397\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7002 - accuracy: 0.7445 - val_loss: 0.7000 - val_accuracy: 0.7483\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7034 - accuracy: 0.7428 - val_loss: 0.7388 - val_accuracy: 0.7283\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7005 - accuracy: 0.7446 - val_loss: 0.6984 - val_accuracy: 0.7498\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6987 - accuracy: 0.7450 - val_loss: 0.7657 - val_accuracy: 0.7115\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7030 - accuracy: 0.7436 - val_loss: 0.7012 - val_accuracy: 0.7495\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6950 - accuracy: 0.7461 - val_loss: 0.6859 - val_accuracy: 0.7563\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6958 - accuracy: 0.7455 - val_loss: 0.7072 - val_accuracy: 0.7458\n",
      "Epoch 43/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6960 - accuracy: 0.7460 - val_loss: 0.7138 - val_accuracy: 0.7451\n",
      "Epoch 44/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6945 - accuracy: 0.7486 - val_loss: 0.6813 - val_accuracy: 0.7519\n",
      "Epoch 45/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6936 - accuracy: 0.7446 - val_loss: 0.7006 - val_accuracy: 0.7472\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6886 - accuracy: 0.7482 - val_loss: 0.6920 - val_accuracy: 0.7492\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6908 - accuracy: 0.7483 - val_loss: 0.7000 - val_accuracy: 0.7439\n",
      "Epoch 48/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6869 - accuracy: 0.7487 - val_loss: 0.7121 - val_accuracy: 0.7486\n",
      "Epoch 49/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6896 - accuracy: 0.7483 - val_loss: 0.6972 - val_accuracy: 0.7523\n",
      "Epoch 50/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6865 - accuracy: 0.7487 - val_loss: 0.7069 - val_accuracy: 0.7426\n",
      "Epoch 51/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6859 - accuracy: 0.7505 - val_loss: 0.6804 - val_accuracy: 0.7577\n",
      "Epoch 52/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6908 - accuracy: 0.7484 - val_loss: 0.6738 - val_accuracy: 0.7614\n",
      "Epoch 53/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6805 - accuracy: 0.7522 - val_loss: 0.6935 - val_accuracy: 0.7518\n",
      "Epoch 54/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6802 - accuracy: 0.7527 - val_loss: 0.7092 - val_accuracy: 0.7439\n",
      "Epoch 55/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6833 - accuracy: 0.7502 - val_loss: 0.7194 - val_accuracy: 0.7358\n",
      "Epoch 56/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6821 - accuracy: 0.7496 - val_loss: 0.6700 - val_accuracy: 0.7601\n",
      "Epoch 57/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6786 - accuracy: 0.7511 - val_loss: 0.6770 - val_accuracy: 0.7610\n",
      "Epoch 58/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6780 - accuracy: 0.7527 - val_loss: 0.6940 - val_accuracy: 0.7466\n",
      "Epoch 59/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6782 - accuracy: 0.7519 - val_loss: 0.7062 - val_accuracy: 0.7377\n",
      "Epoch 60/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6742 - accuracy: 0.7535 - val_loss: 0.6699 - val_accuracy: 0.7613\n",
      "Epoch 61/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6780 - accuracy: 0.7526 - val_loss: 0.6748 - val_accuracy: 0.7577\n",
      "Epoch 62/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6702 - accuracy: 0.7555 - val_loss: 0.7008 - val_accuracy: 0.7485\n",
      "Epoch 63/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6768 - accuracy: 0.7528 - val_loss: 0.6899 - val_accuracy: 0.7497\n",
      "Epoch 64/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6737 - accuracy: 0.7534 - val_loss: 0.7591 - val_accuracy: 0.7237\n",
      "Epoch 65/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6740 - accuracy: 0.7544 - val_loss: 0.7081 - val_accuracy: 0.7431\n",
      "Epoch 66/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6745 - accuracy: 0.7533 - val_loss: 0.6601 - val_accuracy: 0.7648\n",
      "Epoch 67/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6722 - accuracy: 0.7550 - val_loss: 0.7093 - val_accuracy: 0.7402\n",
      "Epoch 68/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6715 - accuracy: 0.7545 - val_loss: 0.6637 - val_accuracy: 0.7648\n",
      "Epoch 69/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6702 - accuracy: 0.7557 - val_loss: 0.6890 - val_accuracy: 0.7513\n",
      "Epoch 70/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6704 - accuracy: 0.7549 - val_loss: 0.6878 - val_accuracy: 0.7547\n",
      "Epoch 71/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6713 - accuracy: 0.7545 - val_loss: 0.6830 - val_accuracy: 0.7594\n",
      "Epoch 72/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6663 - accuracy: 0.7569 - val_loss: 0.6762 - val_accuracy: 0.7603\n",
      "Epoch 73/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6662 - accuracy: 0.7561 - val_loss: 0.6772 - val_accuracy: 0.7563\n",
      "Epoch 74/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6687 - accuracy: 0.7554 - val_loss: 0.7009 - val_accuracy: 0.7517\n",
      "Epoch 75/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6643 - accuracy: 0.7566 - val_loss: 0.6738 - val_accuracy: 0.7598\n",
      "Epoch 76/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6670 - accuracy: 0.7567 - val_loss: 0.6876 - val_accuracy: 0.7486\n",
      "Epoch 77/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6662 - accuracy: 0.7548 - val_loss: 0.7138 - val_accuracy: 0.7459\n",
      "Epoch 78/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6652 - accuracy: 0.7553 - val_loss: 0.6829 - val_accuracy: 0.7602\n",
      "Epoch 79/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6609 - accuracy: 0.7578 - val_loss: 0.6950 - val_accuracy: 0.7412\n",
      "Epoch 80/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6600 - accuracy: 0.7566 - val_loss: 0.6792 - val_accuracy: 0.7592\n",
      "Epoch 81/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6622 - accuracy: 0.7578 - val_loss: 0.6697 - val_accuracy: 0.7600\n",
      "Epoch 82/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6632 - accuracy: 0.7589 - val_loss: 0.6586 - val_accuracy: 0.7600\n",
      "Epoch 83/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6587 - accuracy: 0.7577 - val_loss: 0.6818 - val_accuracy: 0.7496\n",
      "Epoch 84/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6581 - accuracy: 0.7599 - val_loss: 0.6710 - val_accuracy: 0.7585\n",
      "Epoch 85/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6618 - accuracy: 0.7581 - val_loss: 0.6431 - val_accuracy: 0.7667\n",
      "Epoch 86/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6560 - accuracy: 0.7598 - val_loss: 0.6882 - val_accuracy: 0.7520\n",
      "Epoch 87/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6618 - accuracy: 0.7585 - val_loss: 0.7428 - val_accuracy: 0.7240\n",
      "Epoch 88/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6583 - accuracy: 0.7591 - val_loss: 0.6587 - val_accuracy: 0.7624\n",
      "Epoch 89/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6581 - accuracy: 0.7593 - val_loss: 0.6521 - val_accuracy: 0.7662\n",
      "Epoch 90/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6567 - accuracy: 0.7589 - val_loss: 0.6576 - val_accuracy: 0.7642\n",
      "Epoch 91/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6552 - accuracy: 0.7608 - val_loss: 0.6681 - val_accuracy: 0.7609\n",
      "Epoch 92/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6532 - accuracy: 0.7617 - val_loss: 0.6567 - val_accuracy: 0.7653\n",
      "Epoch 93/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6558 - accuracy: 0.7586 - val_loss: 0.7046 - val_accuracy: 0.7446\n",
      "Epoch 94/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6519 - accuracy: 0.7599 - val_loss: 0.6607 - val_accuracy: 0.7598\n",
      "Epoch 95/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6552 - accuracy: 0.7616 - val_loss: 0.6545 - val_accuracy: 0.7638\n",
      "Epoch 96/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6542 - accuracy: 0.7602 - val_loss: 0.6949 - val_accuracy: 0.7395\n",
      "Epoch 97/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6561 - accuracy: 0.7588 - val_loss: 0.6557 - val_accuracy: 0.7636\n",
      "Epoch 98/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6534 - accuracy: 0.7592 - val_loss: 0.7123 - val_accuracy: 0.7387\n",
      "Epoch 99/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6527 - accuracy: 0.7601 - val_loss: 0.6512 - val_accuracy: 0.7648\n",
      "Epoch 100/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6529 - accuracy: 0.7601 - val_loss: 0.6619 - val_accuracy: 0.7602\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_features_flat,\n",
    "    train_label,\n",
    "    epochs=100,\n",
    "    validation_data=(val_features_flat, valid_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "LJO2gSnu4kcf",
    "outputId": "bba9a93a-1862-480f-f969-cc5c48785419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 75.23000240325928\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_features_flat,test_Y_one_hot, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UaHLKDLas_dF"
   },
   "source": [
    "### Task 1.4 Performance comparison\n",
    "\n",
    "*(weight ~2%)*\n",
    "\n",
    "Record the test accuracy achieved at different training configurations above. Which method achieved the highest accuracy? Why did it work better for this problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KqEjlgT-UtEd"
   },
   "source": [
    "|Model Name  | Optimiser  | Learning Rate  | Number of Epochs   |  Test Accuracy |\n",
    "|---|---|---|---|---|\n",
    "|  DNN | Adam  | 0.0001  | 200  | 88.55  |\n",
    "|  ConvNet | Adam  | -  |  100 | 91.43  | \n",
    "|  ConvNet + Dropout | Adam   | -  |  100 | 93.05  | \n",
    "|  ConvNet + Data Augmentation | Adam  |  - | 100  | 91.75  | \n",
    "|  VGG16 without Fine Tuning | Adam  | -  | 100  | 72.3  |  \n",
    "|  VGG16 withoutout Fine Tuning | Adam  | -  | 100  | 75.23  |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UR-f9wAZX01"
   },
   "source": [
    "The best model was **ConvNet with Dropout** which gave the highest test accuracy out of all the model implement, which is **93.05** . Because the outputs of a layer under dropout are randomly subsampled, it has the effect of reducing the capacity or thinning the network during training therfore reducing overfitting and increasing test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ouK5NY-_pLDK"
   },
   "source": [
    "## Task 2 Fast training of deep networks\n",
    "\n",
    "*(weight ~20%)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgoOE2W1pdfN"
   },
   "source": [
    "### Task 2.1 Train a highly accurate network for CIFAR10\n",
    "\n",
    "*(weight ~7%)*\n",
    "\n",
    "In this task, you will train deep neural networks on the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). Compared with the datasets that you have worked on so far, CIFAR10 represents a relatively larger multi-class classification problem and presents a great opportunity for you to solve a \"harder\" problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IaD5oqj3lhuI"
   },
   "source": [
    "#### Task 2.1.1 Document the hardware used\n",
    "\n",
    "Before you start, write down your hardware specifications, including \n",
    "\n",
    "- the GPU model, the number of GPUs, and the GPU memory\n",
    "- the CPU model, the number of CPUs, and the CPU clock speed\n",
    "\n",
    "(Hint: you may find commands like `nvidia-smi`, `lscpu` or `psutil` useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "diTP4rz7lKC9",
    "outputId": "f7a78416-feff-46be-dede-87db443b2f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-6e6ea200-1a0f-1f54-3d4e-7ed14c9e8489)\n"
     ]
    }
   ],
   "source": [
    "#GPU model\n",
    "\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "rDyoT0y9l6KW",
    "outputId": "4c62ba9c-85c6-48ba-da16-2e9f81ee82a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: /physical_device:GPU:0   Type: GPU\n"
     ]
    }
   ],
   "source": [
    "#Number of GPU's with their name :\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAJgmBF91hii"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "AP9XWnU4iYuo",
    "outputId": "146531c1-7b4c-44f3-efc1-6cd694118047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gputil\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
      "Building wheels for collected packages: gputil\n",
      "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=d4572e634a7c6686f700063b0bc06e445792e37bd60fe3946eb673fc376cdd0d\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
      "Successfully built gputil\n",
      "Installing collected packages: gputil\n",
      "Successfully installed gputil-1.4.0\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
      "Gen RAM Free: 24.6 GB  | Proc size: 8.6 GB\n",
      "GPU RAM Free: 11635MB | Used: 4645MB | Util  29% | Total 16280MB\n"
     ]
    }
   ],
   "source": [
    "# Checking GPU's memory \n",
    "#Solution from stack overflow : https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
    "\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "Rnb6yb9WgwTF",
    "outputId": "ff93d7e5-76d6-41d1-fee0-d7c85ee437ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n"
     ]
    }
   ],
   "source": [
    "#CPU model\n",
    "!lscpu |grep 'Model name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "UDozE6_RljlR",
    "outputId": "7e2e94b7-d7ff-4c3a-df6d-69b475fa102b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of CPUs\n",
    "\n",
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LwcV4ByXoVDC",
    "outputId": "cdecf6d6-9344-4ecb-f44f-6fb15abd378f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor\t: 0\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 79\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "stepping\t: 0\n",
      "microcode\t: 0x1\n",
      "cpu MHz\t\t: 2200.000\n",
      "cache size\t: 56320 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 4\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 2\n",
      "apicid\t\t: 0\n",
      "initial apicid\t: 0\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 13\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit\n",
      "bogomips\t: 4400.00\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 1\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 79\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "stepping\t: 0\n",
      "microcode\t: 0x1\n",
      "cpu MHz\t\t: 2200.000\n",
      "cache size\t: 56320 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 4\n",
      "core id\t\t: 1\n",
      "cpu cores\t: 2\n",
      "apicid\t\t: 2\n",
      "initial apicid\t: 2\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 13\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit\n",
      "bogomips\t: 4400.00\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 2\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 79\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "stepping\t: 0\n",
      "microcode\t: 0x1\n",
      "cpu MHz\t\t: 2200.000\n",
      "cache size\t: 56320 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 4\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 2\n",
      "apicid\t\t: 1\n",
      "initial apicid\t: 1\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 13\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit\n",
      "bogomips\t: 4400.00\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 3\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 79\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "stepping\t: 0\n",
      "microcode\t: 0x1\n",
      "cpu MHz\t\t: 2200.000\n",
      "cache size\t: 56320 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 4\n",
      "core id\t\t: 1\n",
      "cpu cores\t: 2\n",
      "apicid\t\t: 3\n",
      "initial apicid\t: 3\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 13\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit\n",
      "bogomips\t: 4400.00\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CPU clock speed can be found here \n",
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adN9Tq-6lyG-"
   },
   "source": [
    "#### Task 2.1.2 Train a \"shallow\" ConvNet\n",
    "\n",
    "Build a ConvNet with fewer than 10 layers. Train the network until it converges. You will use this network as a baseline for the later experiments. \n",
    "\n",
    "- Plot the training and validation history. \n",
    "- Report the testing accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "ax0TyDmJNhTA",
    "outputId": "0e3576ef-6c25-4edd-fff7-f9fa42b4a28d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Loading the CIFAR10 dataset\n",
    "(trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "#One hot encode target values\n",
    "trainY = to_categorical(trainY)\n",
    "testY = to_categorical(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1ASCw-TqBjX"
   },
   "outputs": [],
   "source": [
    "#Normalizing and changing to float type\n",
    "\n",
    "def preprocessing(train, test):\n",
    "\t\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\n",
    "\treturn train_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQblxqt5qKb-"
   },
   "outputs": [],
   "source": [
    "#Decided to go with 10 layers by adding a dropout layer after the dense layer, otherwise the overfitting was too much\n",
    "\n",
    "def define_model():\n",
    "  \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HnXuv8rqRQw"
   },
   "outputs": [],
   "source": [
    "def plots(history):\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  epochs = range(1, len(acc) + 1)\n",
    "\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.plot(epochs, acc, 'red', label='Training acc')\n",
    "  plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.figure()\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.plot(epochs, loss, 'red', label='Training loss')\n",
    "  plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pm96DrM_t-OO"
   },
   "outputs": [],
   "source": [
    "trainX, testX = preprocessing(trainX, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "3-ntTLtdqdg3",
    "outputId": "0420b428-ccca-4a41-92e5-008986d33d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_90 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_91 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_92 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 356,810\n",
      "Trainable params: 356,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1xlfVaPVNiPi",
    "outputId": "d3a193a7-10a5-4932-8c63-c36f94c4944a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0285 - accuracy: 0.2617 - val_loss: 1.7265 - val_accuracy: 0.4029\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7449 - accuracy: 0.3730 - val_loss: 1.5572 - val_accuracy: 0.4429\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6086 - accuracy: 0.4221 - val_loss: 1.4604 - val_accuracy: 0.4836\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5189 - accuracy: 0.4597 - val_loss: 1.3591 - val_accuracy: 0.5276\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4482 - accuracy: 0.4838 - val_loss: 1.2953 - val_accuracy: 0.5503\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3840 - accuracy: 0.5054 - val_loss: 1.2572 - val_accuracy: 0.5588\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3298 - accuracy: 0.5264 - val_loss: 1.1937 - val_accuracy: 0.5865\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2860 - accuracy: 0.5449 - val_loss: 1.1739 - val_accuracy: 0.5883\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2444 - accuracy: 0.5600 - val_loss: 1.1229 - val_accuracy: 0.6115\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.2084 - accuracy: 0.5750 - val_loss: 1.0979 - val_accuracy: 0.6176\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1728 - accuracy: 0.5895 - val_loss: 1.1214 - val_accuracy: 0.6102\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1417 - accuracy: 0.5982 - val_loss: 1.1024 - val_accuracy: 0.6136\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1175 - accuracy: 0.6096 - val_loss: 1.0709 - val_accuracy: 0.6255\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0847 - accuracy: 0.6190 - val_loss: 1.0484 - val_accuracy: 0.6314\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0697 - accuracy: 0.6264 - val_loss: 1.0447 - val_accuracy: 0.6316\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.0464 - accuracy: 0.6342 - val_loss: 0.9925 - val_accuracy: 0.6551\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0272 - accuracy: 0.6407 - val_loss: 1.0490 - val_accuracy: 0.6264\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0074 - accuracy: 0.6496 - val_loss: 0.9646 - val_accuracy: 0.6651\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9928 - accuracy: 0.6547 - val_loss: 0.9483 - val_accuracy: 0.6737\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9692 - accuracy: 0.6618 - val_loss: 0.9724 - val_accuracy: 0.6598\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9556 - accuracy: 0.6684 - val_loss: 0.9814 - val_accuracy: 0.6529\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9417 - accuracy: 0.6721 - val_loss: 0.9349 - val_accuracy: 0.6761\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9256 - accuracy: 0.6787 - val_loss: 0.9298 - val_accuracy: 0.6761\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9082 - accuracy: 0.6866 - val_loss: 0.9078 - val_accuracy: 0.6882\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8910 - accuracy: 0.6917 - val_loss: 0.9041 - val_accuracy: 0.6886\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8755 - accuracy: 0.6958 - val_loss: 0.8916 - val_accuracy: 0.6905\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.8681 - accuracy: 0.7005 - val_loss: 0.9277 - val_accuracy: 0.6792\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8538 - accuracy: 0.7041 - val_loss: 0.8721 - val_accuracy: 0.6953\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8405 - accuracy: 0.7083 - val_loss: 0.9469 - val_accuracy: 0.6704\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8291 - accuracy: 0.7120 - val_loss: 0.8896 - val_accuracy: 0.6907\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8117 - accuracy: 0.7185 - val_loss: 0.8947 - val_accuracy: 0.6848\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8046 - accuracy: 0.7203 - val_loss: 0.8612 - val_accuracy: 0.6991\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7960 - accuracy: 0.7234 - val_loss: 0.8574 - val_accuracy: 0.6962\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7796 - accuracy: 0.7296 - val_loss: 0.9070 - val_accuracy: 0.6871\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7730 - accuracy: 0.7314 - val_loss: 0.8417 - val_accuracy: 0.7057\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7601 - accuracy: 0.7364 - val_loss: 0.8594 - val_accuracy: 0.7016\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7499 - accuracy: 0.7405 - val_loss: 0.8441 - val_accuracy: 0.7068\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7396 - accuracy: 0.7426 - val_loss: 0.8466 - val_accuracy: 0.7030\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7294 - accuracy: 0.7480 - val_loss: 0.8375 - val_accuracy: 0.7105\n",
      "Epoch 40/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7203 - accuracy: 0.7505 - val_loss: 0.8215 - val_accuracy: 0.7135\n",
      "Epoch 41/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7100 - accuracy: 0.7527 - val_loss: 0.8128 - val_accuracy: 0.7145\n",
      "Epoch 42/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6999 - accuracy: 0.7589 - val_loss: 0.8545 - val_accuracy: 0.7091\n",
      "Epoch 43/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6916 - accuracy: 0.7601 - val_loss: 0.8173 - val_accuracy: 0.7143\n",
      "Epoch 44/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6842 - accuracy: 0.7628 - val_loss: 0.8592 - val_accuracy: 0.7028\n",
      "Epoch 45/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6743 - accuracy: 0.7681 - val_loss: 0.8142 - val_accuracy: 0.7135\n",
      "Epoch 46/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6694 - accuracy: 0.7678 - val_loss: 0.8108 - val_accuracy: 0.7206\n",
      "Epoch 47/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6544 - accuracy: 0.7722 - val_loss: 0.7965 - val_accuracy: 0.7240\n",
      "Epoch 48/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6451 - accuracy: 0.7755 - val_loss: 0.8225 - val_accuracy: 0.7170\n",
      "Epoch 49/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6373 - accuracy: 0.7790 - val_loss: 0.8136 - val_accuracy: 0.7210\n",
      "Epoch 50/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6324 - accuracy: 0.7820 - val_loss: 0.7959 - val_accuracy: 0.7254\n",
      "Epoch 51/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6299 - accuracy: 0.7823 - val_loss: 0.8013 - val_accuracy: 0.7264\n",
      "Epoch 52/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6192 - accuracy: 0.7861 - val_loss: 0.8630 - val_accuracy: 0.7088\n",
      "Epoch 53/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6064 - accuracy: 0.7889 - val_loss: 0.8200 - val_accuracy: 0.7168\n",
      "Epoch 54/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6018 - accuracy: 0.7925 - val_loss: 0.8117 - val_accuracy: 0.7283\n",
      "Epoch 55/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5906 - accuracy: 0.7949 - val_loss: 0.8317 - val_accuracy: 0.7227\n",
      "Epoch 56/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5856 - accuracy: 0.7994 - val_loss: 0.8220 - val_accuracy: 0.7216\n",
      "Epoch 57/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5800 - accuracy: 0.7989 - val_loss: 0.8099 - val_accuracy: 0.7275\n",
      "Epoch 58/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5738 - accuracy: 0.8006 - val_loss: 0.8053 - val_accuracy: 0.7313\n",
      "Epoch 59/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5641 - accuracy: 0.8062 - val_loss: 0.8144 - val_accuracy: 0.7281\n",
      "Epoch 60/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5546 - accuracy: 0.8063 - val_loss: 0.8117 - val_accuracy: 0.7314\n",
      "Epoch 61/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5509 - accuracy: 0.8084 - val_loss: 0.8072 - val_accuracy: 0.7288\n",
      "Epoch 62/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5441 - accuracy: 0.8115 - val_loss: 0.7929 - val_accuracy: 0.7330\n",
      "Epoch 63/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5374 - accuracy: 0.8130 - val_loss: 0.8001 - val_accuracy: 0.7306\n",
      "Epoch 64/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5293 - accuracy: 0.8152 - val_loss: 0.8289 - val_accuracy: 0.7280\n",
      "Epoch 65/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5246 - accuracy: 0.8159 - val_loss: 0.8367 - val_accuracy: 0.7272\n",
      "Epoch 66/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5233 - accuracy: 0.8182 - val_loss: 0.8447 - val_accuracy: 0.7178\n",
      "Epoch 67/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5169 - accuracy: 0.8200 - val_loss: 0.8388 - val_accuracy: 0.7253\n",
      "Epoch 68/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5076 - accuracy: 0.8231 - val_loss: 0.8184 - val_accuracy: 0.7340\n",
      "Epoch 69/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4999 - accuracy: 0.8254 - val_loss: 0.8401 - val_accuracy: 0.7263\n",
      "Epoch 70/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4986 - accuracy: 0.8246 - val_loss: 0.7962 - val_accuracy: 0.7343\n",
      "Epoch 71/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4950 - accuracy: 0.8285 - val_loss: 0.7965 - val_accuracy: 0.7306\n",
      "Epoch 72/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4875 - accuracy: 0.8303 - val_loss: 0.8027 - val_accuracy: 0.7416\n",
      "Epoch 73/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4825 - accuracy: 0.8302 - val_loss: 0.8476 - val_accuracy: 0.7357\n",
      "Epoch 74/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4769 - accuracy: 0.8328 - val_loss: 0.8459 - val_accuracy: 0.7311\n",
      "Epoch 75/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4732 - accuracy: 0.8354 - val_loss: 0.8118 - val_accuracy: 0.7330\n",
      "Epoch 76/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4643 - accuracy: 0.8381 - val_loss: 0.8365 - val_accuracy: 0.7430\n",
      "Epoch 77/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4610 - accuracy: 0.8392 - val_loss: 0.9044 - val_accuracy: 0.7175\n",
      "Epoch 78/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4509 - accuracy: 0.8421 - val_loss: 0.8607 - val_accuracy: 0.7323\n",
      "Epoch 79/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4516 - accuracy: 0.8423 - val_loss: 0.8331 - val_accuracy: 0.7342\n",
      "Epoch 80/100\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4487 - accuracy: 0.8428 - val_loss: 0.8175 - val_accuracy: 0.7382\n",
      "Epoch 81/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4413 - accuracy: 0.8467 - val_loss: 0.7966 - val_accuracy: 0.7359\n",
      "Epoch 82/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4389 - accuracy: 0.8473 - val_loss: 0.8182 - val_accuracy: 0.7400\n",
      "Epoch 83/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4355 - accuracy: 0.8490 - val_loss: 0.8932 - val_accuracy: 0.7284\n",
      "Epoch 84/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4337 - accuracy: 0.8487 - val_loss: 0.8417 - val_accuracy: 0.7336\n",
      "Epoch 85/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4252 - accuracy: 0.8528 - val_loss: 0.8385 - val_accuracy: 0.7391\n",
      "Epoch 86/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4210 - accuracy: 0.8542 - val_loss: 0.8272 - val_accuracy: 0.7352\n",
      "Epoch 87/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4180 - accuracy: 0.8559 - val_loss: 0.8876 - val_accuracy: 0.7348\n",
      "Epoch 88/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4120 - accuracy: 0.8558 - val_loss: 0.8572 - val_accuracy: 0.7375\n",
      "Epoch 89/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4097 - accuracy: 0.8575 - val_loss: 0.8438 - val_accuracy: 0.7372\n",
      "Epoch 90/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4052 - accuracy: 0.8592 - val_loss: 0.8465 - val_accuracy: 0.7419\n",
      "Epoch 91/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4033 - accuracy: 0.8606 - val_loss: 0.8794 - val_accuracy: 0.7330\n",
      "Epoch 92/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3983 - accuracy: 0.8611 - val_loss: 0.9075 - val_accuracy: 0.7348\n",
      "Epoch 93/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3989 - accuracy: 0.8601 - val_loss: 0.8716 - val_accuracy: 0.7328\n",
      "Epoch 94/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3871 - accuracy: 0.8650 - val_loss: 0.8915 - val_accuracy: 0.7360\n",
      "Epoch 95/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3887 - accuracy: 0.8639 - val_loss: 0.8709 - val_accuracy: 0.7358\n",
      "Epoch 96/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3866 - accuracy: 0.8660 - val_loss: 0.9249 - val_accuracy: 0.7411\n",
      "Epoch 97/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3848 - accuracy: 0.8658 - val_loss: 0.9102 - val_accuracy: 0.7376\n",
      "Epoch 98/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3777 - accuracy: 0.8696 - val_loss: 0.9333 - val_accuracy: 0.7383\n",
      "Epoch 99/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3743 - accuracy: 0.8697 - val_loss: 0.8668 - val_accuracy: 0.7396\n",
      "Epoch 100/100\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3749 - accuracy: 0.8690 - val_loss: 0.9508 - val_accuracy: 0.7409\n",
      "CPU times: user 7min 54s, sys: 2min 5s, total: 9min 59s\n",
      "Wall time: 9min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit(trainX, trainY, epochs=100, batch_size=128, validation_data=(testX, testY), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "colab_type": "code",
    "id": "4JgY0aLUNigT",
    "outputId": "478702b5-45c8-4fc7-b355-1688f0b79884"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "LpoHjDgK44jP",
    "outputId": "b443f177-a520-4b00-a4f8-b2dfa0c69602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 74.08999800682068\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testX, testY, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9NTjVIUkmv7S"
   },
   "source": [
    "#### Task 2.1.3 Train a ResNet\n",
    "\n",
    "Train a residual neural network (ResNet) on the CIFAR10 training data and report the test accuracy and the training time.\n",
    "\n",
    "The ResNet is a popular network architecture for image classification. You may find more information about how ResNet works by reading this [paper](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "\n",
    "*(You may implement a resnet model or use an existing implementation. In either case, you should not use pretrained network weights.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkiDYDVrNeld"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                        padding=\"SAME\", \n",
    "                        use_bias=False,\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                        )\n",
    "\n",
    "class ResidualUnit(keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            keras.layers.BatchNormalization()]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                keras.layers.BatchNormalization()]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "ZKEJ9_9qNe2R",
    "outputId": "90c8b70e-f398-440f-f2ee-7da2c937860d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 32, 32, 64)        1728      \n",
      "_________________________________________________________________\n",
      "residual_unit_4 (ResidualUni (None, 32, 32, 64)        74240     \n",
      "_________________________________________________________________\n",
      "residual_unit_5 (ResidualUni (None, 16, 16, 128)       230912    \n",
      "_________________________________________________________________\n",
      "residual_unit_6 (ResidualUni (None, 8, 8, 256)         920576    \n",
      "_________________________________________________________________\n",
      "residual_unit_7 (ResidualUni (None, 4, 4, 256)         1248256   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 2,478,282\n",
      "Trainable params: 2,474,186\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "prev_filters = 64\n",
    "c = [prev_filters, 2*prev_filters, 4*prev_filters, 4*prev_filters]\n",
    "model.add(DefaultConv2D(c[0], \n",
    "                        input_shape=[32, 32, 3]))\n",
    "model.add(ResidualUnit(c[0], strides=1))\n",
    "model.add(ResidualUnit(c[1], strides=2))\n",
    "model.add(ResidualUnit(c[2], strides=2))\n",
    "model.add(ResidualUnit(c[3], strides=2))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=(4, 4)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gb3T9hHiNfLp",
    "outputId": "3c1c51ff-8a8e-4f57-b456-21a754683551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 15s 150ms/step - loss: 19.1158 - accuracy: 0.3097 - val_loss: 18.9817 - val_accuracy: 0.1011\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 15s 145ms/step - loss: 17.4747 - accuracy: 0.4253 - val_loss: 18.2699 - val_accuracy: 0.0999\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 15s 145ms/step - loss: 16.2027 - accuracy: 0.4792 - val_loss: 17.4941 - val_accuracy: 0.1054\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 15.0580 - accuracy: 0.5121 - val_loss: 16.0250 - val_accuracy: 0.1852\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 15s 147ms/step - loss: 13.8514 - accuracy: 0.5932 - val_loss: 14.0527 - val_accuracy: 0.3814\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 13.0031 - accuracy: 0.5994 - val_loss: 13.0182 - val_accuracy: 0.4572\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 12.2322 - accuracy: 0.6067 - val_loss: 12.2985 - val_accuracy: 0.4810\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 11.4972 - accuracy: 0.6328 - val_loss: 11.4234 - val_accuracy: 0.5340\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 15s 145ms/step - loss: 10.6165 - accuracy: 0.7192 - val_loss: 10.7895 - val_accuracy: 0.5494\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 10.0707 - accuracy: 0.7166 - val_loss: 10.2388 - val_accuracy: 0.5641\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 9.5997 - accuracy: 0.7108 - val_loss: 9.8150 - val_accuracy: 0.5585\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 9.0872 - accuracy: 0.7348 - val_loss: 9.2422 - val_accuracy: 0.5988\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 8.4091 - accuracy: 0.8230 - val_loss: 8.7861 - val_accuracy: 0.5955\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 8.0342 - accuracy: 0.8092 - val_loss: 8.4985 - val_accuracy: 0.5871\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 7.6890 - accuracy: 0.7993 - val_loss: 8.0135 - val_accuracy: 0.6204\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 15s 145ms/step - loss: 7.2819 - accuracy: 0.8343 - val_loss: 7.9857 - val_accuracy: 0.5490\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 6.7733 - accuracy: 0.8975 - val_loss: 7.3719 - val_accuracy: 0.6095\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 6.4705 - accuracy: 0.8811 - val_loss: 7.1122 - val_accuracy: 0.6056\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 6.2248 - accuracy: 0.8595 - val_loss: 6.7799 - val_accuracy: 0.6237\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 14s 145ms/step - loss: 5.8751 - accuracy: 0.9072 - val_loss: 6.4933 - val_accuracy: 0.6322\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 5.4826 - accuracy: 0.9363 - val_loss: 6.2209 - val_accuracy: 0.6300\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 5.2446 - accuracy: 0.9244 - val_loss: 5.9975 - val_accuracy: 0.6273\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 5.0466 - accuracy: 0.9095 - val_loss: 5.9111 - val_accuracy: 0.5960\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 15s 145ms/step - loss: 4.7645 - accuracy: 0.9398 - val_loss: 5.6421 - val_accuracy: 0.6039\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 4.4617 - accuracy: 0.9588 - val_loss: 5.5113 - val_accuracy: 0.5809\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 4.2879 - accuracy: 0.9438 - val_loss: 5.1184 - val_accuracy: 0.6323\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 15s 145ms/step - loss: 4.1472 - accuracy: 0.9309 - val_loss: 5.2311 - val_accuracy: 0.5839\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 14s 145ms/step - loss: 3.9089 - accuracy: 0.9609 - val_loss: 4.7431 - val_accuracy: 0.6419\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 15s 145ms/step - loss: 3.7036 - accuracy: 0.9632 - val_loss: 4.8739 - val_accuracy: 0.5969\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 15s 145ms/step - loss: 3.5803 - accuracy: 0.9495 - val_loss: 4.5430 - val_accuracy: 0.6277\n",
      "CPU times: user 4min 8s, sys: 1min 26s, total: 5min 34s\n",
      "Wall time: 7min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(trainX, trainY, steps_per_epoch=100, epochs=30, batch_size=128, validation_data=(testX, testY), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "jgqpvbF5NfVg",
    "outputId": "90235b38-e561-4939-c6fe-c6463fc2bbaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 62.769997119903564\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testX, testY, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AH6ZBiECzS75"
   },
   "source": [
    "### Task 2.2 Fast training of ResNet\n",
    "\n",
    "*(weight ~10%)*\n",
    "\n",
    "In this task, you will experiment with different ways to reduce the time for training your ResNet on CIFAR10. There are different ways to speed up neural network training; below are two ideas. Please select at least one idea to implement. Explain the experiment steps and report the final performance and training time.\n",
    "\n",
    "#### Option 1. Learning rate schedule\n",
    "\n",
    "Use a learning rate schedule for the training. Some popular learning rate schedules include \n",
    "\n",
    "- the Step Decay learning rate (e.g., see [here](https://github.com/kuangliu/pytorch-cifar))\n",
    "- [Cyclical learning rates](https://arxiv.org/abs/1506.01186)\n",
    "- [The exponential learning rate](https://openreview.net/forum?id=rJg8TeSFDH) \n",
    "\n",
    "Also Keras provides [some convenient functions](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules) that you can use.\n",
    "\n",
    "\n",
    "#### Option 2. Look ahead optimiser\n",
    "\n",
    "Read [this paper](https://arxiv.org/abs/1907.08610) and implement the Lookahead optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOUK0nWr4N3j"
   },
   "outputs": [],
   "source": [
    "# I used the OPTION 1 :  learning rate scheduler method (step decay learning rate) to implement ResNet on Cifar10 dataset\n",
    "# Help from https://github.com/sukilau/Ziff-deep-learning/blob/master/3-CIFAR10-lrate/CIFAR10-lrate.ipynb\n",
    "\n",
    "\n",
    "#Step decay tends to drop the learning rate according to the epochs \n",
    "#This function is taken as an augment by the Learning Sheduler method, while it returns the updated Learning Rates to the SGD optimiser i am using\n",
    "\n",
    "def step_decay(epoch):\n",
    "\n",
    "    lr = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lr = lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    print('Learning rate: ', lr)\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zwRtXk7L796l"
   },
   "outputs": [],
   "source": [
    "#Custom callback by extending the base class to callbacks to record the loss history and and learning rate during the experiment\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "        print('lr:', step_decay(len(self.losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "-xyDLq45qyTD",
    "outputId": "68ad987b-c11c-478d-c5c9-9dcd42c45654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_73 (Conv2D)           (None, 32, 32, 64)        1728      \n",
      "_________________________________________________________________\n",
      "residual_unit_20 (ResidualUn (None, 32, 32, 64)        74240     \n",
      "_________________________________________________________________\n",
      "residual_unit_21 (ResidualUn (None, 16, 16, 128)       230912    \n",
      "_________________________________________________________________\n",
      "residual_unit_22 (ResidualUn (None, 8, 8, 256)         920576    \n",
      "_________________________________________________________________\n",
      "residual_unit_23 (ResidualUn (None, 4, 4, 256)         1248256   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 2,478,282\n",
      "Trainable params: 2,474,186\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#ResNet model Implementation \n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "prev_filters = 64\n",
    "c = [prev_filters, 2*prev_filters, 4*prev_filters, 4*prev_filters]\n",
    "model.add(DefaultConv2D(c[0], \n",
    "                        input_shape=[32, 32, 3]))\n",
    "model.add(ResidualUnit(c[0], strides=1))\n",
    "model.add(ResidualUnit(c[1], strides=2))\n",
    "model.add(ResidualUnit(c[2], strides=2))\n",
    "model.add(ResidualUnit(c[3], strides=2))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=(4, 4)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPaYA3JC4X5F"
   },
   "outputs": [],
   "source": [
    "#Setting the initial learning to 0.0 as it will changed in the step decay function\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=0.0, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lwKQUziy4cjT"
   },
   "outputs": [],
   "source": [
    "#Custom callback function\n",
    "loss_history = LossHistory()\n",
    "\n",
    "#Learning Rate Scheduler function callback for dropping the learning rate into half every 10 epochs\n",
    "lr_scheduler = LearningRateScheduler(step_decay)\n",
    "\n",
    "callbacks = [loss_history, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Kz8sM4MS6_7o",
    "outputId": "58695086-1ec8-408e-a686-a112165f0f14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 10.3853 - accuracy: 0.6004Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "lr: 0.001\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 10.3815 - accuracy: 0.6006 - val_loss: 11.1883 - val_accuracy: 0.3867 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 2/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 10.0981 - accuracy: 0.5919Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "lr: 0.001\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 10.0975 - accuracy: 0.5913 - val_loss: 10.2613 - val_accuracy: 0.4830 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 3/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 9.7852 - accuracy: 0.5751Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "lr: 0.001\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 9.7839 - accuracy: 0.5738 - val_loss: 9.9457 - val_accuracy: 0.4801 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 4/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 9.4448 - accuracy: 0.5874Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "lr: 0.001\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 9.4415 - accuracy: 0.5891 - val_loss: 9.7261 - val_accuracy: 0.4634 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 5/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 9.1434 - accuracy: 0.5900Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "lr: 0.001\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 9.1416 - accuracy: 0.5900 - val_loss: 9.3198 - val_accuracy: 0.4936 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 6/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 8.7791 - accuracy: 0.6133Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "lr: 0.001\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 8.7789 - accuracy: 0.6134 - val_loss: 8.9305 - val_accuracy: 0.5117 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 7/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 8.5147 - accuracy: 0.6105Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "lr: 0.001\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 8.5092 - accuracy: 0.6119 - val_loss: 8.5728 - val_accuracy: 0.5376 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 8/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 8.2599 - accuracy: 0.5960Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "lr: 0.001\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 8.2568 - accuracy: 0.5966 - val_loss: 8.5048 - val_accuracy: 0.4902 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 9/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 7.9783 - accuracy: 0.6146Learning rate:  0.0005\n",
      "Learning rate:  0.0005\n",
      "lr: 0.0005\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 7.9782 - accuracy: 0.6144 - val_loss: 8.2014 - val_accuracy: 0.4739 - lr: 0.0010\n",
      "Learning rate:  0.0005\n",
      "Epoch 10/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 7.7225 - accuracy: 0.6212Learning rate:  0.0005\n",
      "Learning rate:  0.0005\n",
      "lr: 0.0005\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 7.7222 - accuracy: 0.6209 - val_loss: 7.7753 - val_accuracy: 0.5844 - lr: 5.0000e-04\n",
      "Learning rate:  0.0005\n",
      "Epoch 11/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 7.5025 - accuracy: 0.6619Learning rate:  0.0005\n",
      "Learning rate:  0.0005\n",
      "lr: 0.0005\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 7.4994 - accuracy: 0.6634 - val_loss: 7.6404 - val_accuracy: 0.5898 - lr: 5.0000e-04\n",
      "Learning rate:  0.0005\n",
      "Epoch 12/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 7.3862 - accuracy: 0.6512Learning rate:  0.0005\n",
      "Learning rate:  0.0005\n",
      "lr: 0.0005\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 7.3847 - accuracy: 0.6516 - val_loss: 7.5629 - val_accuracy: 0.5850 - lr: 5.0000e-04\n",
      "Learning rate:  0.0005\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 7.2724 - accuracy: 0.6515Learning rate:  0.0005\n",
      "Learning rate:  0.0005\n",
      "lr: 0.0005\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 7.2718 - accuracy: 0.6519 - val_loss: 7.2736 - val_accuracy: 0.6278 - lr: 5.0000e-04\n",
      "Learning rate:  0.0005\n",
      "Epoch 14/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 7.0860 - accuracy: 0.6761Learning rate:  0.0005\n",
      "Learning rate:  0.0005\n",
      "lr: 0.0005\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 7.0853 - accuracy: 0.6766 - val_loss: 7.0984 - val_accuracy: 0.6463 - lr: 5.0000e-04\n",
      "Learning rate:  0.0005\n",
      "Epoch 15/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 6.9653 - accuracy: 0.6723Learning rate:  0.0005\n",
      "Learning rate:  0.0005\n",
      "lr: 0.0005\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 6.9653 - accuracy: 0.6725 - val_loss: 7.1506 - val_accuracy: 0.6011 - lr: 5.0000e-04\n",
      "Learning rate:  0.0005\n",
      "Epoch 16/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 6.7909 - accuracy: 0.6856Learning rate:  0.0005\n",
      "Learning rate:  0.0005\n",
      "lr: 0.0005\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 6.7863 - accuracy: 0.6875 - val_loss: 6.8866 - val_accuracy: 0.6421 - lr: 5.0000e-04\n",
      "Learning rate:  0.0005\n",
      "Epoch 17/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 6.5396 - accuracy: 0.7459Learning rate:  0.0005\n",
      "Learning rate:  0.0005\n",
      "lr: 0.0005\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 6.5378 - accuracy: 0.7469 - val_loss: 6.8044 - val_accuracy: 0.6345 - lr: 5.0000e-04\n",
      "Learning rate:  0.0005\n",
      "Epoch 18/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 6.4297 - accuracy: 0.7497Learning rate:  0.0005\n",
      "Learning rate:  0.0005\n",
      "lr: 0.0005\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 6.4279 - accuracy: 0.7503 - val_loss: 6.8686 - val_accuracy: 0.5827 - lr: 5.0000e-04\n",
      "Learning rate:  0.0005\n",
      "Epoch 19/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 6.3196 - accuracy: 0.7478Learning rate:  0.00025\n",
      "Learning rate:  0.00025\n",
      "lr: 0.00025\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 6.3191 - accuracy: 0.7472 - val_loss: 6.6879 - val_accuracy: 0.6130 - lr: 5.0000e-04\n",
      "Learning rate:  0.00025\n",
      "Epoch 20/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 6.2471 - accuracy: 0.7424Learning rate:  0.00025\n",
      "Learning rate:  0.00025\n",
      "lr: 0.00025\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 6.2466 - accuracy: 0.7434 - val_loss: 6.4028 - val_accuracy: 0.6837 - lr: 2.5000e-04\n",
      "Learning rate:  0.00025\n",
      "Epoch 21/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 6.1088 - accuracy: 0.7718Learning rate:  0.00025\n",
      "Learning rate:  0.00025\n",
      "lr: 0.00025\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 6.1103 - accuracy: 0.7719 - val_loss: 6.3409 - val_accuracy: 0.6804 - lr: 2.5000e-04\n",
      "Learning rate:  0.00025\n",
      "Epoch 22/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 6.0344 - accuracy: 0.7860Learning rate:  0.00025\n",
      "Learning rate:  0.00025\n",
      "lr: 0.00025\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 6.0328 - accuracy: 0.7869 - val_loss: 6.2619 - val_accuracy: 0.6891 - lr: 2.5000e-04\n",
      "Learning rate:  0.00025\n",
      "Epoch 23/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 6.0352 - accuracy: 0.7652Learning rate:  0.00025\n",
      "Learning rate:  0.00025\n",
      "lr: 0.00025\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 6.0336 - accuracy: 0.7663 - val_loss: 6.1992 - val_accuracy: 0.6907 - lr: 2.5000e-04\n",
      "Learning rate:  0.00025\n",
      "Epoch 24/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.9493 - accuracy: 0.7724Learning rate:  0.00025\n",
      "Learning rate:  0.00025\n",
      "lr: 0.00025\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.9465 - accuracy: 0.7731 - val_loss: 6.1603 - val_accuracy: 0.6884 - lr: 2.5000e-04\n",
      "Learning rate:  0.00025\n",
      "Epoch 25/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.8871 - accuracy: 0.7860Learning rate:  0.00025\n",
      "Learning rate:  0.00025\n",
      "lr: 0.00025\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 5.8889 - accuracy: 0.7859 - val_loss: 6.0843 - val_accuracy: 0.6970 - lr: 2.5000e-04\n",
      "Learning rate:  0.00025\n",
      "Epoch 26/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.8686 - accuracy: 0.7674Learning rate:  0.00025\n",
      "Learning rate:  0.00025\n",
      "lr: 0.00025\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.8696 - accuracy: 0.7678 - val_loss: 6.0546 - val_accuracy: 0.6906 - lr: 2.5000e-04\n",
      "Learning rate:  0.00025\n",
      "Epoch 27/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.8037 - accuracy: 0.7740Learning rate:  0.00025\n",
      "Learning rate:  0.00025\n",
      "lr: 0.00025\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 5.8029 - accuracy: 0.7744 - val_loss: 5.9824 - val_accuracy: 0.7024 - lr: 2.5000e-04\n",
      "Learning rate:  0.00025\n",
      "Epoch 28/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.7663 - accuracy: 0.7693Learning rate:  0.00025\n",
      "Learning rate:  0.00025\n",
      "lr: 0.00025\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.7656 - accuracy: 0.7684 - val_loss: 5.9619 - val_accuracy: 0.6894 - lr: 2.5000e-04\n",
      "Learning rate:  0.00025\n",
      "Epoch 29/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.6683 - accuracy: 0.7872Learning rate:  0.000125\n",
      "Learning rate:  0.000125\n",
      "lr: 0.000125\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.6688 - accuracy: 0.7862 - val_loss: 5.9039 - val_accuracy: 0.6942 - lr: 2.5000e-04\n",
      "Learning rate:  0.000125\n",
      "Epoch 30/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.6260 - accuracy: 0.7895Learning rate:  0.000125\n",
      "Learning rate:  0.000125\n",
      "lr: 0.000125\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.6260 - accuracy: 0.7897 - val_loss: 5.8387 - val_accuracy: 0.7101 - lr: 1.2500e-04\n",
      "Learning rate:  0.000125\n",
      "Epoch 31/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.5849 - accuracy: 0.7936Learning rate:  0.000125\n",
      "Learning rate:  0.000125\n",
      "lr: 0.000125\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.5863 - accuracy: 0.7931 - val_loss: 5.8034 - val_accuracy: 0.7081 - lr: 1.2500e-04\n",
      "Learning rate:  0.000125\n",
      "Epoch 32/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.4231 - accuracy: 0.8563Learning rate:  0.000125\n",
      "Learning rate:  0.000125\n",
      "lr: 0.000125\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.4226 - accuracy: 0.8558 - val_loss: 5.7510 - val_accuracy: 0.7201 - lr: 1.2500e-04\n",
      "Learning rate:  0.000125\n",
      "Epoch 33/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.3622 - accuracy: 0.8744Learning rate:  0.000125\n",
      "Learning rate:  0.000125\n",
      "lr: 0.000125\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.3626 - accuracy: 0.8737 - val_loss: 5.7258 - val_accuracy: 0.7192 - lr: 1.2500e-04\n",
      "Learning rate:  0.000125\n",
      "Epoch 34/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.3238 - accuracy: 0.8731Learning rate:  0.000125\n",
      "Learning rate:  0.000125\n",
      "lr: 0.000125\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.3229 - accuracy: 0.8731 - val_loss: 5.7107 - val_accuracy: 0.7186 - lr: 1.2500e-04\n",
      "Learning rate:  0.000125\n",
      "Epoch 35/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.3025 - accuracy: 0.8728Learning rate:  0.000125\n",
      "Learning rate:  0.000125\n",
      "lr: 0.000125\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.3019 - accuracy: 0.8731 - val_loss: 5.6765 - val_accuracy: 0.7220 - lr: 1.2500e-04\n",
      "Learning rate:  0.000125\n",
      "Epoch 36/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.3035 - accuracy: 0.8703Learning rate:  0.000125\n",
      "Learning rate:  0.000125\n",
      "lr: 0.000125\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.3037 - accuracy: 0.8700 - val_loss: 5.6613 - val_accuracy: 0.7206 - lr: 1.2500e-04\n",
      "Learning rate:  0.000125\n",
      "Epoch 37/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.2637 - accuracy: 0.8693Learning rate:  0.000125\n",
      "Learning rate:  0.000125\n",
      "lr: 0.000125\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 5.2617 - accuracy: 0.8700 - val_loss: 5.6376 - val_accuracy: 0.7188 - lr: 1.2500e-04\n",
      "Learning rate:  0.000125\n",
      "Epoch 38/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.2391 - accuracy: 0.8646Learning rate:  0.000125\n",
      "Learning rate:  0.000125\n",
      "lr: 0.000125\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.2386 - accuracy: 0.8647 - val_loss: 5.6040 - val_accuracy: 0.7229 - lr: 1.2500e-04\n",
      "Learning rate:  0.000125\n",
      "Epoch 39/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.2414 - accuracy: 0.8583Learning rate:  6.25e-05\n",
      "Learning rate:  6.25e-05\n",
      "lr: 6.25e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.2405 - accuracy: 0.8572 - val_loss: 5.5798 - val_accuracy: 0.7228 - lr: 1.2500e-04\n",
      "Learning rate:  6.25e-05\n",
      "Epoch 40/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.1815 - accuracy: 0.8750Learning rate:  6.25e-05\n",
      "Learning rate:  6.25e-05\n",
      "lr: 6.25e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.1831 - accuracy: 0.8744 - val_loss: 5.5604 - val_accuracy: 0.7241 - lr: 6.2500e-05\n",
      "Learning rate:  6.25e-05\n",
      "Epoch 41/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.1743 - accuracy: 0.8753Learning rate:  6.25e-05\n",
      "Learning rate:  6.25e-05\n",
      "lr: 6.25e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.1753 - accuracy: 0.8744 - val_loss: 5.5384 - val_accuracy: 0.7278 - lr: 6.2500e-05\n",
      "Learning rate:  6.25e-05\n",
      "Epoch 42/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.1763 - accuracy: 0.8699Learning rate:  6.25e-05\n",
      "Learning rate:  6.25e-05\n",
      "lr: 6.25e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.1762 - accuracy: 0.8700 - val_loss: 5.5337 - val_accuracy: 0.7277 - lr: 6.2500e-05\n",
      "Learning rate:  6.25e-05\n",
      "Epoch 43/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.1439 - accuracy: 0.8741Learning rate:  6.25e-05\n",
      "Learning rate:  6.25e-05\n",
      "lr: 6.25e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.1423 - accuracy: 0.8744 - val_loss: 5.5113 - val_accuracy: 0.7297 - lr: 6.2500e-05\n",
      "Learning rate:  6.25e-05\n",
      "Epoch 44/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.1444 - accuracy: 0.8734Learning rate:  6.25e-05\n",
      "Learning rate:  6.25e-05\n",
      "lr: 6.25e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.1444 - accuracy: 0.8737 - val_loss: 5.5079 - val_accuracy: 0.7268 - lr: 6.2500e-05\n",
      "Learning rate:  6.25e-05\n",
      "Epoch 45/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.1246 - accuracy: 0.8690Learning rate:  6.25e-05\n",
      "Learning rate:  6.25e-05\n",
      "lr: 6.25e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.1241 - accuracy: 0.8691 - val_loss: 5.4985 - val_accuracy: 0.7289 - lr: 6.2500e-05\n",
      "Learning rate:  6.25e-05\n",
      "Epoch 46/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.1031 - accuracy: 0.8804Learning rate:  6.25e-05\n",
      "Learning rate:  6.25e-05\n",
      "lr: 6.25e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.1019 - accuracy: 0.8806 - val_loss: 5.4729 - val_accuracy: 0.7307 - lr: 6.2500e-05\n",
      "Learning rate:  6.25e-05\n",
      "Epoch 47/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.1011 - accuracy: 0.8782Learning rate:  6.25e-05\n",
      "Learning rate:  6.25e-05\n",
      "lr: 6.25e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.0997 - accuracy: 0.8788 - val_loss: 5.4663 - val_accuracy: 0.7332 - lr: 6.2500e-05\n",
      "Learning rate:  6.25e-05\n",
      "Epoch 48/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.0144 - accuracy: 0.9100Learning rate:  6.25e-05\n",
      "Learning rate:  6.25e-05\n",
      "lr: 6.25e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.0134 - accuracy: 0.9106 - val_loss: 5.4538 - val_accuracy: 0.7302 - lr: 6.2500e-05\n",
      "Learning rate:  6.25e-05\n",
      "Epoch 49/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9896 - accuracy: 0.9211Learning rate:  3.125e-05\n",
      "Learning rate:  3.125e-05\n",
      "lr: 3.125e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9881 - accuracy: 0.9216 - val_loss: 5.4439 - val_accuracy: 0.7305 - lr: 6.2500e-05\n",
      "Learning rate:  3.125e-05\n",
      "Epoch 50/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 5.0054 - accuracy: 0.9062Learning rate:  3.125e-05\n",
      "Learning rate:  3.125e-05\n",
      "lr: 3.125e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 5.0059 - accuracy: 0.9056 - val_loss: 5.4320 - val_accuracy: 0.7346 - lr: 3.1250e-05\n",
      "Learning rate:  3.125e-05\n",
      "Epoch 51/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9836 - accuracy: 0.9126Learning rate:  3.125e-05\n",
      "Learning rate:  3.125e-05\n",
      "lr: 3.125e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9837 - accuracy: 0.9122 - val_loss: 5.4244 - val_accuracy: 0.7351 - lr: 3.1250e-05\n",
      "Learning rate:  3.125e-05\n",
      "Epoch 52/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9598 - accuracy: 0.9195Learning rate:  3.125e-05\n",
      "Learning rate:  3.125e-05\n",
      "lr: 3.125e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9596 - accuracy: 0.9194 - val_loss: 5.4185 - val_accuracy: 0.7326 - lr: 3.1250e-05\n",
      "Learning rate:  3.125e-05\n",
      "Epoch 53/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9531 - accuracy: 0.9148Learning rate:  3.125e-05\n",
      "Learning rate:  3.125e-05\n",
      "lr: 3.125e-05\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 4.9553 - accuracy: 0.9137 - val_loss: 5.4084 - val_accuracy: 0.7331 - lr: 3.1250e-05\n",
      "Learning rate:  3.125e-05\n",
      "Epoch 54/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9771 - accuracy: 0.9107Learning rate:  3.125e-05\n",
      "Learning rate:  3.125e-05\n",
      "lr: 3.125e-05\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 4.9781 - accuracy: 0.9094 - val_loss: 5.4037 - val_accuracy: 0.7344 - lr: 3.1250e-05\n",
      "Learning rate:  3.125e-05\n",
      "Epoch 55/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9609 - accuracy: 0.9075Learning rate:  3.125e-05\n",
      "Learning rate:  3.125e-05\n",
      "lr: 3.125e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9604 - accuracy: 0.9081 - val_loss: 5.3974 - val_accuracy: 0.7341 - lr: 3.1250e-05\n",
      "Learning rate:  3.125e-05\n",
      "Epoch 56/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9461 - accuracy: 0.9154Learning rate:  3.125e-05\n",
      "Learning rate:  3.125e-05\n",
      "lr: 3.125e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9467 - accuracy: 0.9156 - val_loss: 5.3957 - val_accuracy: 0.7328 - lr: 3.1250e-05\n",
      "Learning rate:  3.125e-05\n",
      "Epoch 57/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9408 - accuracy: 0.9104Learning rate:  3.125e-05\n",
      "Learning rate:  3.125e-05\n",
      "lr: 3.125e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9397 - accuracy: 0.9106 - val_loss: 5.3853 - val_accuracy: 0.7362 - lr: 3.1250e-05\n",
      "Learning rate:  3.125e-05\n",
      "Epoch 58/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9283 - accuracy: 0.9201Learning rate:  3.125e-05\n",
      "Learning rate:  3.125e-05\n",
      "lr: 3.125e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9292 - accuracy: 0.9194 - val_loss: 5.3826 - val_accuracy: 0.7351 - lr: 3.1250e-05\n",
      "Learning rate:  3.125e-05\n",
      "Epoch 59/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9401 - accuracy: 0.9050Learning rate:  1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "lr: 1.5625e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9434 - accuracy: 0.9031 - val_loss: 5.3753 - val_accuracy: 0.7340 - lr: 3.1250e-05\n",
      "Learning rate:  1.5625e-05\n",
      "Epoch 60/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9246 - accuracy: 0.9141Learning rate:  1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "lr: 1.5625e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9246 - accuracy: 0.9137 - val_loss: 5.3695 - val_accuracy: 0.7359 - lr: 1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "Epoch 61/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9219 - accuracy: 0.9107Learning rate:  1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "lr: 1.5625e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9237 - accuracy: 0.9100 - val_loss: 5.3662 - val_accuracy: 0.7362 - lr: 1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "Epoch 62/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9130 - accuracy: 0.9138Learning rate:  1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "lr: 1.5625e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9125 - accuracy: 0.9141 - val_loss: 5.3649 - val_accuracy: 0.7348 - lr: 1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "Epoch 63/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.9028 - accuracy: 0.9210Learning rate:  1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "lr: 1.5625e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.9034 - accuracy: 0.9205 - val_loss: 5.3613 - val_accuracy: 0.7365 - lr: 1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "Epoch 64/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8981 - accuracy: 0.9186Learning rate:  1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "lr: 1.5625e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8979 - accuracy: 0.9184 - val_loss: 5.3586 - val_accuracy: 0.7377 - lr: 1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "Epoch 65/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8816 - accuracy: 0.9274Learning rate:  1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "lr: 1.5625e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8816 - accuracy: 0.9275 - val_loss: 5.3551 - val_accuracy: 0.7365 - lr: 1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "Epoch 66/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8595 - accuracy: 0.9340Learning rate:  1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "lr: 1.5625e-05\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8614 - accuracy: 0.9337 - val_loss: 5.3534 - val_accuracy: 0.7351 - lr: 1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "Epoch 67/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8655 - accuracy: 0.9340Learning rate:  1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "lr: 1.5625e-05\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 4.8653 - accuracy: 0.9337 - val_loss: 5.3509 - val_accuracy: 0.7359 - lr: 1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "Epoch 68/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8799 - accuracy: 0.9328Learning rate:  1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "lr: 1.5625e-05\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 4.8797 - accuracy: 0.9331 - val_loss: 5.3473 - val_accuracy: 0.7364 - lr: 1.5625e-05\n",
      "Learning rate:  1.5625e-05\n",
      "Epoch 69/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8773 - accuracy: 0.9265Learning rate:  7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "lr: 7.8125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8788 - accuracy: 0.9253 - val_loss: 5.3433 - val_accuracy: 0.7358 - lr: 1.5625e-05\n",
      "Learning rate:  7.8125e-06\n",
      "Epoch 70/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8728 - accuracy: 0.9249Learning rate:  7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "lr: 7.8125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8723 - accuracy: 0.9250 - val_loss: 5.3412 - val_accuracy: 0.7364 - lr: 7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "Epoch 71/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8710 - accuracy: 0.9280Learning rate:  7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "lr: 7.8125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8704 - accuracy: 0.9284 - val_loss: 5.3397 - val_accuracy: 0.7366 - lr: 7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "Epoch 72/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8645 - accuracy: 0.9299Learning rate:  7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "lr: 7.8125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8646 - accuracy: 0.9297 - val_loss: 5.3389 - val_accuracy: 0.7358 - lr: 7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "Epoch 73/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8698 - accuracy: 0.9255Learning rate:  7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "lr: 7.8125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8703 - accuracy: 0.9253 - val_loss: 5.3383 - val_accuracy: 0.7349 - lr: 7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "Epoch 74/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8561 - accuracy: 0.9309Learning rate:  7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "lr: 7.8125e-06\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 4.8569 - accuracy: 0.9306 - val_loss: 5.3364 - val_accuracy: 0.7359 - lr: 7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "Epoch 75/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8598 - accuracy: 0.9306Learning rate:  7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "lr: 7.8125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8591 - accuracy: 0.9309 - val_loss: 5.3343 - val_accuracy: 0.7369 - lr: 7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "Epoch 76/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8615 - accuracy: 0.9249Learning rate:  7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "lr: 7.8125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8608 - accuracy: 0.9253 - val_loss: 5.3326 - val_accuracy: 0.7362 - lr: 7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "Epoch 77/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8577 - accuracy: 0.9287Learning rate:  7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "lr: 7.8125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8576 - accuracy: 0.9284 - val_loss: 5.3302 - val_accuracy: 0.7369 - lr: 7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "Epoch 78/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8662 - accuracy: 0.9217Learning rate:  7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "lr: 7.8125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8666 - accuracy: 0.9219 - val_loss: 5.3293 - val_accuracy: 0.7372 - lr: 7.8125e-06\n",
      "Learning rate:  7.8125e-06\n",
      "Epoch 79/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8410 - accuracy: 0.9372Learning rate:  3.90625e-06\n",
      "Learning rate:  3.90625e-06\n",
      "lr: 3.90625e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8398 - accuracy: 0.9375 - val_loss: 5.3279 - val_accuracy: 0.7370 - lr: 7.8125e-06\n",
      "Learning rate:  3.90625e-06\n",
      "Epoch 80/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8519 - accuracy: 0.9324Learning rate:  3.90625e-06\n",
      "Learning rate:  3.90625e-06\n",
      "lr: 3.90625e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8505 - accuracy: 0.9328 - val_loss: 5.3267 - val_accuracy: 0.7371 - lr: 3.9063e-06\n",
      "Learning rate:  3.90625e-06\n",
      "Epoch 81/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8508 - accuracy: 0.9318Learning rate:  3.90625e-06\n",
      "Learning rate:  3.90625e-06\n",
      "lr: 3.90625e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8506 - accuracy: 0.9319 - val_loss: 5.3258 - val_accuracy: 0.7365 - lr: 3.9063e-06\n",
      "Learning rate:  3.90625e-06\n",
      "Epoch 82/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8459 - accuracy: 0.9350Learning rate:  3.90625e-06\n",
      "Learning rate:  3.90625e-06\n",
      "lr: 3.90625e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8457 - accuracy: 0.9353 - val_loss: 5.3253 - val_accuracy: 0.7364 - lr: 3.9063e-06\n",
      "Learning rate:  3.90625e-06\n",
      "Epoch 83/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8238 - accuracy: 0.9410Learning rate:  3.90625e-06\n",
      "Learning rate:  3.90625e-06\n",
      "lr: 3.90625e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8235 - accuracy: 0.9416 - val_loss: 5.3250 - val_accuracy: 0.7367 - lr: 3.9063e-06\n",
      "Learning rate:  3.90625e-06\n",
      "Epoch 84/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8420 - accuracy: 0.9265Learning rate:  3.90625e-06\n",
      "Learning rate:  3.90625e-06\n",
      "lr: 3.90625e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8422 - accuracy: 0.9266 - val_loss: 5.3249 - val_accuracy: 0.7367 - lr: 3.9063e-06\n",
      "Learning rate:  3.90625e-06\n",
      "Epoch 85/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8452 - accuracy: 0.9277Learning rate:  3.90625e-06\n",
      "Learning rate:  3.90625e-06\n",
      "lr: 3.90625e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8449 - accuracy: 0.9281 - val_loss: 5.3237 - val_accuracy: 0.7366 - lr: 3.9063e-06\n",
      "Learning rate:  3.90625e-06\n",
      "Epoch 86/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8347 - accuracy: 0.9318Learning rate:  3.90625e-06\n",
      "Learning rate:  3.90625e-06\n",
      "lr: 3.90625e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8341 - accuracy: 0.9322 - val_loss: 5.3233 - val_accuracy: 0.7375 - lr: 3.9063e-06\n",
      "Learning rate:  3.90625e-06\n",
      "Epoch 87/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8261 - accuracy: 0.9375Learning rate:  3.90625e-06\n",
      "Learning rate:  3.90625e-06\n",
      "lr: 3.90625e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8252 - accuracy: 0.9378 - val_loss: 5.3220 - val_accuracy: 0.7378 - lr: 3.9063e-06\n",
      "Learning rate:  3.90625e-06\n",
      "Epoch 88/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8356 - accuracy: 0.9350Learning rate:  3.90625e-06\n",
      "Learning rate:  3.90625e-06\n",
      "lr: 3.90625e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8355 - accuracy: 0.9350 - val_loss: 5.3212 - val_accuracy: 0.7376 - lr: 3.9063e-06\n",
      "Learning rate:  3.90625e-06\n",
      "Epoch 89/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8387 - accuracy: 0.9293Learning rate:  1.953125e-06\n",
      "Learning rate:  1.953125e-06\n",
      "lr: 1.953125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8381 - accuracy: 0.9294 - val_loss: 5.3202 - val_accuracy: 0.7376 - lr: 3.9063e-06\n",
      "Learning rate:  1.953125e-06\n",
      "Epoch 90/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8409 - accuracy: 0.9290Learning rate:  1.953125e-06\n",
      "Learning rate:  1.953125e-06\n",
      "lr: 1.953125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8416 - accuracy: 0.9291 - val_loss: 5.3200 - val_accuracy: 0.7377 - lr: 1.9531e-06\n",
      "Learning rate:  1.953125e-06\n",
      "Epoch 91/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8305 - accuracy: 0.9378Learning rate:  1.953125e-06\n",
      "Learning rate:  1.953125e-06\n",
      "lr: 1.953125e-06\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 4.8297 - accuracy: 0.9378 - val_loss: 5.3197 - val_accuracy: 0.7378 - lr: 1.9531e-06\n",
      "Learning rate:  1.953125e-06\n",
      "Epoch 92/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8331 - accuracy: 0.9328Learning rate:  1.953125e-06\n",
      "Learning rate:  1.953125e-06\n",
      "lr: 1.953125e-06\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 4.8325 - accuracy: 0.9328 - val_loss: 5.3192 - val_accuracy: 0.7373 - lr: 1.9531e-06\n",
      "Learning rate:  1.953125e-06\n",
      "Epoch 93/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8396 - accuracy: 0.9287Learning rate:  1.953125e-06\n",
      "Learning rate:  1.953125e-06\n",
      "lr: 1.953125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8406 - accuracy: 0.9281 - val_loss: 5.3189 - val_accuracy: 0.7365 - lr: 1.9531e-06\n",
      "Learning rate:  1.953125e-06\n",
      "Epoch 94/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8514 - accuracy: 0.9251Learning rate:  1.953125e-06\n",
      "Learning rate:  1.953125e-06\n",
      "lr: 1.953125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8509 - accuracy: 0.9256 - val_loss: 5.3186 - val_accuracy: 0.7365 - lr: 1.9531e-06\n",
      "Learning rate:  1.953125e-06\n",
      "Epoch 95/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8238 - accuracy: 0.9315Learning rate:  1.953125e-06\n",
      "Learning rate:  1.953125e-06\n",
      "lr: 1.953125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8231 - accuracy: 0.9319 - val_loss: 5.3183 - val_accuracy: 0.7369 - lr: 1.9531e-06\n",
      "Learning rate:  1.953125e-06\n",
      "Epoch 96/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8454 - accuracy: 0.9312Learning rate:  1.953125e-06\n",
      "Learning rate:  1.953125e-06\n",
      "lr: 1.953125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8452 - accuracy: 0.9316 - val_loss: 5.3177 - val_accuracy: 0.7371 - lr: 1.9531e-06\n",
      "Learning rate:  1.953125e-06\n",
      "Epoch 97/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8270 - accuracy: 0.9388Learning rate:  1.953125e-06\n",
      "Learning rate:  1.953125e-06\n",
      "lr: 1.953125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8275 - accuracy: 0.9384 - val_loss: 5.3174 - val_accuracy: 0.7366 - lr: 1.9531e-06\n",
      "Learning rate:  1.953125e-06\n",
      "Epoch 98/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8328 - accuracy: 0.9340Learning rate:  1.953125e-06\n",
      "Learning rate:  1.953125e-06\n",
      "lr: 1.953125e-06\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8330 - accuracy: 0.9334 - val_loss: 5.3165 - val_accuracy: 0.7380 - lr: 1.9531e-06\n",
      "Learning rate:  1.953125e-06\n",
      "Epoch 99/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8361 - accuracy: 0.9340Learning rate:  9.765625e-07\n",
      "Learning rate:  9.765625e-07\n",
      "lr: 9.765625e-07\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8355 - accuracy: 0.9341 - val_loss: 5.3167 - val_accuracy: 0.7373 - lr: 1.9531e-06\n",
      "Learning rate:  9.765625e-07\n",
      "Epoch 100/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 4.8190 - accuracy: 0.9426Learning rate:  9.765625e-07\n",
      "Learning rate:  9.765625e-07\n",
      "lr: 9.765625e-07\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 4.8192 - accuracy: 0.9422 - val_loss: 5.3165 - val_accuracy: 0.7374 - lr: 9.7656e-07\n",
      "CPU times: user 8min 50s, sys: 2min 18s, total: 11min 8s\n",
      "Wall time: 13min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#Custom callback and the Learning Rate callback to fit the model\n",
    "history = model.fit(trainX, trainY, steps_per_epoch=100, epochs=100, batch_size=32,callbacks=callbacks, validation_data=(testX, testY), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "NGo-MekSqyBs",
    "outputId": "c025804c-9e75-48ee-c44b-06baf4bd8b41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 73.73999953269958\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testX, testY, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8cKfAOjpn7c"
   },
   "source": [
    "### Task 2.3 Performance comparison\n",
    "\n",
    "*(weight ~3%)*\n",
    "\n",
    "Based on the above experiments, which method or which combination of methods result in the best accuracy with the same training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6e9ftsoJIzoy"
   },
   "source": [
    "|Model Name  | Optimiser  | Learning Rate  | Number of Epochs   |  Test Accuracy | Training Time |\n",
    "|---|---|---|---|---| ---|\n",
    "|  ConvNet < 10 Layers | RMSprop  | 0.0001  | 100  | 74.08  | 9min 34s |\n",
    "|  ResNet | RMSprop  | 0.0001 |  100 | 62.76  | 7min 26s| \n",
    "|  ResNet ( Fast Training ) Learning Scheduler  | SGD   | Step_decay  |  100 | 73.73  | 13min 38s |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXwE-yyOKhKS"
   },
   "source": [
    "It could be seen that the **ConvNet with less than 10 layers** gave the best results, as compared to the ResNet, however this ConvNet could further be improved by adding layers and drop out regularizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUV0wuZ01DNA"
   },
   "source": [
    "---\n",
    "**END OF ASSIGNMENT TWO**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
